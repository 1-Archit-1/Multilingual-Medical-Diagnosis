{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15dfbbb6-2ca8-4d30-932a-bbe62de54c7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "~/scratch/hf-cache\n"
     ]
    }
   ],
   "source": [
    "SHOULD_TRAIN = False\n",
    "#SHOULD_TRAIN = True\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "\n",
    "# disable Weights and Biases\n",
    "os.environ['WANDB_DISABLED']=\"true\"\n",
    "os.environ[\"HF_HOME\"] = \"~/scratch/hf-cache\"\n",
    "token=\"\"\n",
    "print(os.environ['WANDB_DISABLED'])  # Should output \"true\"\n",
    "print(os.environ['HF_HOME'])  # Should output \"~/scratch/hf-cache\"\n",
    "\n",
    "output_file = open('logger_tests2.log', 'w')\n",
    "sys.stdout = output_file\n",
    "sys.stderr = output_file\n",
    "\n",
    "LANG_TOKEN_MAPPING = {\n",
    "    'hi': '',\n",
    "    'en': ''\n",
    "}\n",
    "max_seq_len = 25\n",
    "last_print_time = time.time()\n",
    "model_path = \"./models/\"\n",
    "\n",
    "# Import necessary modules and functions from translate_codebase.py\n",
    "\n",
    "from translate_codebase import (\n",
    "    # Environment variables and settings\n",
    "    # Generic Helper functions\n",
    "    clock_begin,\n",
    "    clock_end,\n",
    "    read_token_and_login,\n",
    "    print_gpu_utilization,\n",
    "    clear_cuda_cache,\n",
    "    # Metrics Functions\n",
    "    evaluate_translations_bertscore,\n",
    "    evaluate_translations_rouge,\n",
    "    evaluate_translations_bleu,\n",
    "    evaluate_translations_meteor,\n",
    "    # Common Data Preprocessing Functions\n",
    "    filter_sentences,\n",
    "    get_reduced_dataset,\n",
    "    prepare_test_data,\n",
    "    perform_translation_testing,\n",
    "    # MBART related functions\n",
    "    get_pretrained_mbart_large_50_many_to_many_mmt,\n",
    "    preprocess_function_mbart,\n",
    "    prepare_model_for_training_mbart,\n",
    "    fine_tune_and_save_mbart,\n",
    "    load_fine_tuned_model_mbart,\n",
    "    translate_text_mbart,\n",
    "    # MT5 related functions\n",
    "    get_pretrained_mt5_small,\n",
    "    config_mt5_small,\n",
    "    encode_input_str_mt5_small,\n",
    "    encode_target_str_mt5_small,\n",
    "    process_translation_list_mt5_small,\n",
    "    format_translation_data_mt5_small,\n",
    "    transform_batch_mt5_small,\n",
    "    fine_tune_and_save_model_mt5_small,\n",
    "    eval_model_mt5_small,\n",
    "    save_fine_tuned_model_mt5_small,\n",
    "    load_fine_tuned_model_mt5_small,\n",
    "    translate_text_mt5_small\n",
    ")\n",
    "\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b961a9-e6be-4b07-a6ad-85ca92ad39d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from team_files.akshay import getDiagnosisLlama\n",
    "from team_files.aditya import load_unsloth_model_and_tokenizer_phi, generate_diagnosis_phi\n",
    "from team_files.archit import load_model_mis, generate_text_mis, inference_mis\n",
    "from team_files.ensemble import ensemble_responses\n",
    "\n",
    "#from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dba5fb8f-1dc3-456a-bdac-5360e8d2cc5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mbart_eng_hin_huggingface(force_download=True):\n",
    "\n",
    "    model_path = \"sathvikaithalkp456/mbart_fine_tuned_eng_hin\"\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(model_path, force_download = force_download)\n",
    "    model = MBartForConditionalGeneration.from_pretrained(model_path, force_download = force_download)\n",
    "    tokenizer.src_lang = \"en_XX\"\n",
    "    tokenizer.tgt_lang = \"hi_IN\"\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_mbart_hin_eng_huggingface(force_download=True):\n",
    "    revision = \"master\"\n",
    "    model_path = \"sathvikaithalkp456/mbart_fine_tuned_hin_eng\"\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(model_path, force_download = force_download, revision=revision)\n",
    "    model = MBartForConditionalGeneration.from_pretrained(model_path, force_download = force_download, revision=revision)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_mt5_small_eng_hin_huggingface(force_download=True):\n",
    "\n",
    "    revision = \"master\"\n",
    "    model_path = 'sathvikaithalkp456/mbart_fine_tuned_eng_hin'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, force_download = force_download, revision=revision)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path, force_download = force_download, revision=revision)\n",
    "    model = model.cuda()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_mt5_small_hin_eng_huggingface(force_download=True):\n",
    "\n",
    "    revision = \"master\"\n",
    "    model_path = 'sathvikaithalkp456/mbart_fine_tuned_hin_eng'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, force_download = force_download, revision=revision)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path, force_download = force_download, revision=revision)\n",
    "    model = model.cuda()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def translate_text_generic(model, tokenizer, input_text, src_lang, tgt_lang, model_type=\"mbart\"):\n",
    "    \"\"\"\n",
    "    Translates a given text from the source language to the target language.\n",
    "\n",
    "    Args:\n",
    "        model: The translation model.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "        input_text: The input text to be translated.\n",
    "        src_lang: The source language code.\n",
    "        tgt_lang: The target language code.\n",
    "        model_type: The type of model (\"mbart\" or \"mt5\").\n",
    "\n",
    "    Returns:\n",
    "        str: The translated text.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    if model_type == \"mbart\":\n",
    "        # Tokenize the input text with padding and truncation\n",
    "        tokenizer.src_lang = src_lang\n",
    "        encoded_input = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "        # Generate translation\n",
    "        generated_tokens = model.generate(\n",
    "            **encoded_input,\n",
    "            forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang]\n",
    "        )\n",
    "\n",
    "        # Decode the generated tokens\n",
    "        translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    else:  # mt5\n",
    "        # Encode the input text with padding and truncation\n",
    "        input_ids = encode_input_str_mt5_small(\n",
    "            text=input_text,\n",
    "            target_lang=tgt_lang,\n",
    "            tokenizer=tokenizer,\n",
    "            seq_len=model.config.max_length,\n",
    "            lang_token_map=LANG_TOKEN_MAPPING\n",
    "        )\n",
    "        input_ids = input_ids.unsqueeze(0).to(device)\n",
    "\n",
    "        # Generate the translation\n",
    "        output_ids = model.generate(input_ids, num_beams=5, max_length=model.config.max_length, early_stopping=True)\n",
    "\n",
    "        # Decode the generated tokens\n",
    "        translated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    #print(\"Translated text:\", translated_text)\n",
    "    return translated_text\n",
    "\n",
    "\n",
    "def translate_sentences_generic(input_lists, output_file, src_lang, tgt_lang, model_type=\"mbart\", direction=\"eng_hin\"):\n",
    "    \"\"\"\n",
    "    Translates each sentence in the input lists to the target language and saves the translated sentences to a file.\n",
    "\n",
    "    Args:\n",
    "        input_lists (list): A list of lists, where each inner list contains sentences to be translated.\n",
    "        output_file (str): The path to the output file where translated sentences will be saved.\n",
    "        src_lang (str): The source language code.\n",
    "        tgt_lang (str): The target language code.\n",
    "        model_type (str): The type of model to use for translation (\"mbart\" or \"mt5\").\n",
    "        direction (str): The direction of translation (\"eng_hin\" or \"hin_eng\").\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if model_type == \"mbart\":\n",
    "        if direction == \"eng_hin\":\n",
    "            model, tokenizer = get_mbart_eng_hin_huggingface()\n",
    "        else:  # hin_eng\n",
    "            model, tokenizer = get_mbart_hin_eng_huggingface()\n",
    "    else:  # mt5\n",
    "        if direction == \"eng_hin\":\n",
    "            model, tokenizer = get_mt5_small_eng_hin_huggingface()\n",
    "        else:  # hin_eng\n",
    "            model, tokenizer = get_mt5_small_hin_eng_huggingface()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    translated_data = []\n",
    "\n",
    "    for input_list in input_lists:\n",
    "        translated_list = []\n",
    "        for sentence in input_list:\n",
    "            translated_sentence = translate_text_generic(model, tokenizer, sentence, src_lang, tgt_lang, model_type=model_type)\n",
    "            translated_list.append(translated_sentence)\n",
    "        translated_data.append(translated_list)\n",
    "\n",
    "    # Save the translated sentences to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(translated_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def parse_input(data):\n",
    "    \"\"\"\n",
    "    Parses the input data, replaces 'Y' with 'Yes' and 'N' with 'No', and returns it as a list of formatted inputs.\n",
    "\n",
    "    Args:\n",
    "        data (str): The input data in JSON format.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of formatted inputs.\n",
    "    \"\"\"\n",
    "    parsed_data = json.loads(data)\n",
    "    formatted_inputs = []\n",
    "\n",
    "    for item in parsed_data:\n",
    "        input_text = item['input']\n",
    "        # Replace ' Y ' with ' Yes ' and ' N ' with ' No '\n",
    "        input_text = input_text.replace(' Y ', ' Yes ').replace(' N ', ' No ')\n",
    "        # Split the input text into individual lines based on delimiters\n",
    "        lines = re.split(r'[;.]', input_text)\n",
    "        # Strip leading and trailing whitespace from each line and filter out empty lines\n",
    "        lines = [line.strip() for line in lines if line.strip()]\n",
    "        formatted_inputs.append(lines)\n",
    "\n",
    "    return formatted_inputs\n",
    "\n",
    "def generate_input():\n",
    "\n",
    "    with open('sample_data.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    data = json.dumps(data)\n",
    "\n",
    "    parsed_inputs = parse_input(data)\n",
    "    print(parsed_inputs)\n",
    "\n",
    "    output_file = \"translated_sentences_hindi.json\"\n",
    "    translate_sentences_generic(parsed_inputs, output_file, src_lang=\"en_XX\", tgt_lang=\"hi_IN\", model_type=\"mbart\", direction=\"eng_hin\")\n",
    "\n",
    "    print(\"Input generated!!\")\n",
    "\n",
    "def get_inputs():\n",
    "    with open('translated_sentences_hindi.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "    print(len(data))\n",
    "    #print(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Function to translate a sample input back to English\n",
    "def translate_sample_to_english(sample_input):\n",
    "    # Wrap the sample input in a list of lists to use with translate_sentences_generic\n",
    "    input_lists = [sample_input]\n",
    "    output_file = \"temp_file.json\"\n",
    "    \n",
    "    # Translate the sample input back to English\n",
    "    translate_sentences_generic(input_lists, output_file, src_lang=\"hi_IN\", tgt_lang=\"en_XX\", model_type=\"mbart\", direction=\"hin_eng\")\n",
    "    \n",
    "    # Load the translated text from the output file\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        translated_data = json.load(f)\n",
    "    \n",
    "    # Extract the translated text\n",
    "    print(\"translated data: \", translated_data)\n",
    "    #translated_text = translated_data\n",
    "    #print(\"Translated text:\", translated_text)\n",
    "    return translated_data\n",
    "\n",
    "def translate_single_sample_to_hindi(diagnosis):\n",
    "    # Wrap the sample input in a list of lists to use with translate_sentences_generic\n",
    "    input_lists = [[diagnosis]]\n",
    "    output_file = \"temp_file_2.json\"\n",
    "    \n",
    "    # Translate the sample input back to English\n",
    "    translate_sentences_generic(input_lists, output_file, src_lang=\"en_XX\", tgt_lang=\"hi_IN\", model_type=\"mbart\", direction=\"eng_hin\")\n",
    "    \n",
    "    # Load the translated text from the output file\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        translated_data = json.load(f)\n",
    "    \n",
    "    # Extract the translated text\n",
    "    print(\"translated data: \", translated_data)\n",
    "    #translated_text = translated_data\n",
    "    #print(\"Translated text:\", translated_text)\n",
    "    return translated_data\n",
    "# Function to translate a sample input back to English\n",
    "def translate_sample_to_hindi(diagnosis,diagnosis_unsloth, diagnosis_mistral):\n",
    "    # Wrap the sample input in a list of lists to use with translate_sentences_generic\n",
    "    input_lists = [[diagnosis],[diagnosis_unsloth], [diagnosis_mistral]]\n",
    "    output_file = \"temp_file_2.json\"\n",
    "    \n",
    "    # Translate the sample input back to English\n",
    "    translate_sentences_generic(input_lists, output_file, src_lang=\"en_XX\", tgt_lang=\"hi_IN\", model_type=\"mbart\", direction=\"eng_hin\")\n",
    "    \n",
    "    # Load the translated text from the output file\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        translated_data = json.load(f)\n",
    "    \n",
    "    # Extract the translated text\n",
    "    print(\"translated data: \", translated_data)\n",
    "    #translated_text = translated_data\n",
    "    #print(\"Translated text:\", translated_text)\n",
    "    return translated_data\n",
    "\n",
    "\n",
    "def process_translated_text_and_get_diagnosis_llama(translated_text):\n",
    "    \"\"\"\n",
    "    Processes the translated text, joins the sentences, and gets the diagnosis from the Medical LLM (Llama).\n",
    "\n",
    "    Args:\n",
    "        translated_text (list): A list of translated sentences.\n",
    "\n",
    "    Returns:\n",
    "        str: The diagnosis from the Medical LLM (Llama).\n",
    "    \"\"\"\n",
    "    # Flatten the list of lists into a single list of strings\n",
    "    flattened_text = [sentence for sublist in translated_text for sentence in sublist]\n",
    "    \n",
    "    # Join the sentences in the translated text\n",
    "    combined_text = ' '.join(flattened_text)\n",
    "    print(f\"Combined text: {combined_text}\")\n",
    "\n",
    "    # Pass the combined text to the Medical LLM and get the diagnosis\n",
    "    diagnosis, diagnosis_json = getDiagnosisLlama(combined_text)\n",
    "    return diagnosis,diagnosis_json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_translated_text_and_get_diagnosis_unsloth(translated_text):\n",
    "    \"\"\"\n",
    "    Processes the translated text, joins the sentences, and gets the diagnosis from the Medical LLM (Unsloth).\n",
    "\n",
    "    Args:\n",
    "        translated_text (list): A list of translated sentences.\n",
    "\n",
    "    Returns:\n",
    "        str: The diagnosis from the Medical LLM (Unsloth).\n",
    "    \"\"\"\n",
    "    # Flatten the list of lists into a single list of strings\n",
    "    flattened_text = [sentence for sublist in translated_text for sentence in sublist]\n",
    "    \n",
    "    # Join the sentences in the translated text\n",
    "    combined_text = ' '.join(flattened_text)\n",
    "    print(f\"Combined text: {combined_text}\")\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    model_path = \"Buddy1421/medical_diagnosis_phi_3-5\"\n",
    "    model, tokenizer = load_unsloth_model_and_tokenizer_phi(model_path, use_safetensors=True)\n",
    "\n",
    "    # Pass the combined text to the Medical LLM and get the diagnosis\n",
    "    diagnosis, diagnosis_json= generate_diagnosis_phi(model, tokenizer, combined_text)\n",
    "    return diagnosis,diagnosis_json\n",
    "\n",
    "\n",
    "def process_translated_text_and_get_diagnosis_mistral(translated_text):\n",
    "    \"\"\"\n",
    "    Processes the translated text, joins the sentences, and gets the diagnosis from the Medical LLM (Mistral).\n",
    "\n",
    "    Args:\n",
    "        translated_text (list): A list of translated sentences.\n",
    "\n",
    "    Returns:\n",
    "        str: The diagnosis from the Medical LLM (Mistral).\n",
    "    \"\"\"\n",
    "    # Flatten the list of lists into a single list of strings\n",
    "    flattened_text = [sentence for sublist in translated_text for sentence in sublist]\n",
    "    \n",
    "    # Join the sentences in the translated text\n",
    "    combined_text = ' '.join(flattened_text)\n",
    "    print(f\"Combined text: {combined_text}\")\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    model, tokenizer = load_model_mis()\n",
    "\n",
    "    # Pass the combined text to the Medical LLM and get the diagnosis\n",
    "    full_response , diagnosis_json = inference_mis(combined_text, max_length=512)\n",
    "    return full_response, diagnosis_json\n",
    "\n",
    "def format_for_ensemble(mistral_json, llama_json, phi_json):\n",
    "    outputs = {\n",
    "        'mistral' :mistral_diagnosis_json,\n",
    "        'llama': phi_json,\n",
    "        'phi': phi_json,\n",
    "    }\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "289aad16-a0de-4498-ba76-cca49292fac9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a35eaca7d44dfea0ed4681d6147838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/10.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430a2a73eb7649b1a26cbcd65f53a449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed09f3f9ff34828a94c110119cdba8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f3aafba83a45e490bc226c30a618aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/992 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7599acca04f547c48afd7cd80e0d95f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/10.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffc007b6c364327954e57b47ffbb2c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c5569b93524e9b9d65aa2ba5c52258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad8332bfb1c4b698ea85531de6888c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fe5212cf134c08b3a03f8713dc15b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/256 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(\"starting demo\")\n",
    "#print(\"generating inputs\")\n",
    "#generate_input()\n",
    "print(\"getting inputs\")\n",
    "inputs = get_inputs()\n",
    "seed = 6\n",
    "print(\"Choosing 1 random input : \",seed, \" from 25 inputs\")\n",
    "test_input = inputs[seed]\n",
    "print(test_input)\n",
    "print(\"Translating to English\")\n",
    "translated_text = translate_sample_to_english(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e1246a7-9e9d-40e6-a5e3-d42574b572e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Calling MEDICAL LLM (Llama)\")\n",
    "diagnosis,llama_diagnosis_json = process_translated_text_and_get_diagnosis_llama(translated_text)\n",
    "print(\"Diagnosis:\", diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f5651fb-c777-4562-af17-00c58643e4c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Calling MEDICAL LLM (PHI)\")\n",
    "diagnosis_unsloth, phi_json = process_translated_text_and_get_diagnosis_unsloth(translated_text)\n",
    "print(\"Diagnosis (Unsloth):\", diagnosis_unsloth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af43325c-e9f3-4a17-90b3-4ed5ef9526df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Calling MEDICAL LLM (Mistral)\")\n",
    "diagnosis_mistral, mistral_diagnosis_json  = process_translated_text_and_get_diagnosis_mistral(translated_text)\n",
    "print(\"Diagnosis (Mistral):\", diagnosis_mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc43dc4a-010d-424d-a2dc-85e6d8e91245",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Translating to Hindi\")\n",
    "translate_sample_to_hindi(diagnosis,diagnosis_unsloth, diagnosis_mistral)\n",
    "\n",
    "# Set up random responses for now, but expect these from the model functions\n",
    "#llama_json = {'most_likely': 'XYZ','differential': ['HIV (initial infection)', 'Colitis', 'jaundice']}\n",
    "#phi_json = {'most_likely': 'XYZ','differential': ['HIV (initial infection)', 'Colitis', 'jaundice']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4789775-4322-405f-8181-b1b558039101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    model_outputs = format_for_ensemble(mistral_diagnosis_json, llama_diagnosis_json, phi_json)\n",
    "    print(\"Final English Response: \")\n",
    "    ensembled = ensemble_responses(model_outputs)\n",
    "    print(ensembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e55dfdd5-84be-4724-abac-04469863da4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f9ba5a25494198ba9035f9ebb85dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/10.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32746a112684028b05fe0f81ae65f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc90b40004549d6bcb5191bde0033bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92cef985a9714ac288e21a39dd47f1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/992 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "822579c85c7a45078993e63d8fb4f0b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/10.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1fface16d1477ead7768826158abf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03fb7f014ac4f7cb09dd49b79e78e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88895f790114655a0777ba3fb9875ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4fe07efda34bc6a9e144554e8f35b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/256 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "translated_final = translate_single_sample_to_hindi(ensembled)\n",
    "print(f'Final Hindi Response {translated_final}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b143445-ff1a-485d-b15e-ddabfd047bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-e2e2]",
   "language": "python",
   "name": "conda-env-.conda-e2e2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
