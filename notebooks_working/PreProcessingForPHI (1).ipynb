{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efe44852",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/akumar978/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/hice1/akumar978/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c7ec458",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers cache directory: /storage/ice1/4/0/akumar978/BDA Project/cache/transformers\n",
      "Torch cache directory: /storage/ice1/4/0/akumar978/BDA Project/cache/torch\n",
      "HuggingFace cache directory: /storage/ice1/4/0/akumar978/BDA Project/cache/huggingface\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Dynamically set scratch folder to the current working directory\n",
    "scratch_folder = os.path.join(os.getcwd(), \"cache\")  # Create a 'cache' folder in your current working directory\n",
    "\n",
    "# Set environment variables for caching\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = os.path.join(scratch_folder, \"transformers\")\n",
    "os.environ[\"TORCH_HOME\"] = os.path.join(scratch_folder, \"torch\")\n",
    "os.environ[\"HF_HOME\"] = os.path.join(scratch_folder, \"huggingface\")\n",
    "\n",
    "# Ensure the cache directories exist\n",
    "os.makedirs(os.environ[\"TRANSFORMERS_CACHE\"], exist_ok=True)\n",
    "os.makedirs(os.environ[\"TORCH_HOME\"], exist_ok=True)\n",
    "os.makedirs(os.environ[\"HF_HOME\"], exist_ok=True)\n",
    "\n",
    "print(f\"Transformers cache directory: {os.environ['TRANSFORMERS_CACHE']}\")\n",
    "print(f\"Torch cache directory: {os.environ['TORCH_HOME']}\")\n",
    "print(f\"HuggingFace cache directory: {os.environ['HF_HOME']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3abbbeae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('release_evidences.json', 'r') as f:\n",
    "    evidence_dict = json.load(f)\n",
    "\n",
    "with open('release_conditions.json', 'r') as f:\n",
    "    condition_dict = json.load(f)\n",
    "    \n",
    "with open('sample_train_data_200k.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "    \n",
    "with open('sampled_test_combined_data.json','r') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f12c909e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Provide Diagnosis', 'input': 'Patient age is 84, sex is M.  Antecedents: Do you have severe Chronic Obstructive Pulmonary Disease (COPD)? Y ; Have you had one or several flare ups of chronic obstructive pulmonary disease (COPD) in the past year? Y ; Do you smoke cigarettes? Y ; Do you have a chronic obstructive pulmonary disease (COPD)? Y ; Have you ever been diagnosed with gastroesophageal reflux? Y ; Do you work in agriculture? Y ; Do you work in construction? Y ; Have you traveled out of the country in the last 4 weeks? N . Symptoms: Do you have a cough that produces colored or more abundant sputum than usual? Y ; Are you experiencing shortness of breath or difficulty breathing in a significant way? Y ; Do you have a cough that produces colored or more abundant sputum than usual? Y ; Do you have a cough? Y ; Have you noticed a wheezing sound when you exhale? Y . ', 'output': ' Differential diagnosis is: Acute COPD exacerbation / infection, Bronchitis, Bronchiectasis, Pneumonia, Pulmonary neoplasm, Guillain-Barre syndrome, Atrial fibrillation, Myocarditis, Pulmonary embolism, Acute dystonic reactions, Myasthenia gravis, Anemia, Tuberculosis, PSVT, Possible NSTEMI / STEMI, Chagas and the Disease can be Acute COPD exacerbation / infection '}\n"
     ]
    }
   ],
   "source": [
    "print(train_data[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2334cd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patients(file_path):\n",
    "    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "        with zip_ref.open(zip_ref.namelist()[0]) as f:\n",
    "            return pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1aff159",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_patients = load_patients('release_train_patients.zip')\n",
    "val_patients = load_patients('release_validate_patients.zip')\n",
    "test_patients = load_patients('release_test_patients.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18e4dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a14867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_representation(row, output_path):\n",
    "    # Gather patient information\n",
    "    age = row['AGE']\n",
    "    sex = row['SEX']\n",
    "    pathology = row['PATHOLOGY']\n",
    "    initial_evidence = row['INITIAL_EVIDENCE']\n",
    "    evidences = eval(row['EVIDENCES'])\n",
    "    evidences = [initial_evidence] + evidences\n",
    "#     For differential diagnosis \n",
    "    data = eval(row['DIFFERENTIAL_DIAGNOSIS'])\n",
    "    differential_diseases = [item[0] for item in data]\n",
    "    diseases = ', '.join(differential_diseases)\n",
    "\n",
    "    description = f\"Age: {age}, Sex: {sex}. \"\n",
    "    # Add detailed symptoms and antecedents\n",
    "    symptom_texts = []\n",
    "    antecedents = []\n",
    "    for evidence_code in evidences:\n",
    "        # Separate multi-choice evidence by value\n",
    "        if \"_@_\" in evidence_code:\n",
    "            evidence, value = evidence_code.split('_@_')\n",
    "            evidence_text = evidence_dict[evidence]['question_en']\n",
    "            value_text = evidence_dict[evidence]['value_meaning'].get(value)\n",
    "            value_text = value_text['en'] if value_text is not None else value\n",
    "            if value_text=='N':\n",
    "                value_text = 'No'\n",
    "            if value_text =='Y':\n",
    "                value_text = 'Yes'\n",
    "            if value_text == 'NA':\n",
    "                value_text = 'Not Applicable'\n",
    "                \n",
    "            if evidence_dict[evidence]['is_antecedent']:\n",
    "                antecedents.append(f\"{evidence_text}: {value_text}\")\n",
    "            else:\n",
    "                symptom_texts.append(f\"{evidence_text}: {value_text}\")\n",
    "        else:\n",
    "            if evidence_dict[evidence_code]['is_antecedent']:\n",
    "                antecedents.append(evidence_dict[evidence_code]['question_en']+'Yes')\n",
    "            else:\n",
    "                symptom_texts.append(evidence_dict[evidence_code]['question_en']+'Yes')\n",
    "\n",
    "    description += \"History:\" + \"; \".join(antecedents) + \". Symptoms: \" + \"; \".join(symptom_texts) + \".\"\n",
    "    label = pathology\n",
    "    \n",
    "    with open(output_path, 'a', encoding='utf-8') as f:\n",
    "        data = {\n",
    "            \"most_likely_disease\": label,\n",
    "            \"differential_diseases\": differential_diseases\n",
    "            \n",
    "        }\n",
    "        # Write each JSON object on a new line without pretty printing\n",
    "        #json.dump(chat_format, f, ensure_ascii=False)\n",
    "        f.write(json.dumps(data)+\"\\n\")  # Add newline after each JSON object\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e26da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9850513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = test_patients.apply(create_text_representation,output_path ='test_result.csv', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5c159d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7719cad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #train_results = pd.DataFrame(train_patients.apply(create_text_representation, axis=1).toList(), columns=['text', 'label'])\n",
    "# val_patient = val_patient\n",
    "# val_result = val_patient.apply(create_text_representation, output_path ='val_result.jsonl', axis=1 )\n",
    "# #val_results = pd.DataFrame(val_results.tolist(), columns=['text', 'label'])\n",
    "\n",
    "# test_patient = test_patient\n",
    "# test_result = test_patient.apply(create_text_representation,output_path ='test_result.jsonl', axis=1)\n",
    "# #test_results = pd.DataFrame(test_results.tolist(), columns=['text', 'label'])\n",
    "\n",
    "# # train_patient = train_patient\n",
    "# train_result = train_patient.apply(create_text_representation,output_path ='train_result.jsonl', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4628a9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_result.to_csv('val_patients_with_text.csv', index=False)\n",
    "# test_result.to_csv('test_patients_with_text.csv', index=False)\n",
    "# train_result.to_csv('train_patients_with_text.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ee47b1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_labels = pd.concat([train_patients['PATHOLOGY'], val_patients['PATHOLOGY'], test_patients['PATHOLOGY']])\n",
    "\n",
    "# # Fit Label Encoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# label_encoder.fit(all_labels)\n",
    "\n",
    "# # Transform pathologies to numerical labels\n",
    "# train_patients_label = label_encoder.transform(train_patients['PATHOLOGY'])\n",
    "# val_patients_label = label_encoder.transform(val_patients['PATHOLOGY'])\n",
    "# test_patients_label = label_encoder.transform(test_patients['PATHOLOGY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daacb4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0d32c593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load and display the first few entries\n",
    "# def inspect_dataset(file_path, num_entries=5):\n",
    "#     try:\n",
    "#         with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#             print(f\"Showing first {num_entries} entries of the dataset:\\n\")\n",
    "#             for i, line in enumerate(f):\n",
    "#                 if i >= num_entries:\n",
    "#                     break\n",
    "#                 # Parse the JSON object from each line\n",
    "#                 chat_format = json.loads(line.strip())\n",
    "#                 print(json.dumps(chat_format, indent=4, ensure_ascii=False))\n",
    "#                 print(\"-\" * 80)  # Separator for clarity\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error while reading the dataset: {e}\")\n",
    "\n",
    "# # Inspect the dataset\n",
    "# inspect_dataset(\"test_result.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8791a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468c7013",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 20:56:35.817266: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-25 20:56:35.829125: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732586195.843877 3263751 cuda_dnn.cc:8321] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732586195.848641 3263751 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-25 20:56:35.865475: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5d7b94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.7: Fast Llama patching. Transformers = 4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA H100 80GB HBM3. Max memory: 79.097 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1+cu124. CUDA = 9.0. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.dev941. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Phi-3.5-mini-instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c82060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "# Load your dataset (assuming it is in JSONL format)\n",
    "data = []\n",
    "with open(\"train_result.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "hf_dataset = Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6a3f5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6000/6000 [00:00<00:00, 34155.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "from datasets import Dataset\n",
    "\n",
    "# Initialize the chat template with your tokenizer\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"phi-3\",  # Specify your chat template\n",
    "    mapping={\n",
    "        \"role\": \"from\",\n",
    "        \"content\": \"value\",\n",
    "        \"user\": \"human\",\n",
    "        \"assistant\": \"gpt\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define the formatting function\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"messages\"]  # Replace with your column name\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "        for convo in convos\n",
    "    ]\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply the formatting function to your dataset\n",
    "formatted_dataset = hf_dataset.map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d8f5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(formatted_dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0a83cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 200026 examples [00:02, 96340.54 examples/s]\n",
      "Generating test split: 9062 examples [00:00, 105260.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files={\"train\": \"sample_train_data_200k.json\", \"test\": \"sampled_test_combined_data.json\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "033ea080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.7: Fast Llama patching. Transformers = 4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA H100 80GB HBM3. Max memory: 79.097 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1+cu124. CUDA = 9.0. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.dev941. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Phi-3.5-mini-instruct\",\n",
    "    max_seq_length = 1024,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81f430fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5236278b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.11.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"o_proj\",\"gate_proj\",\"k_proj\", \"v_proj\",\"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\",    \n",
    "    use_gradient_checkpointing = True,\n",
    "    random_state = 3411,\n",
    "    max_seq_length = 1024,\n",
    "    use_rslora = False,  # Rank stabilized LoRA\n",
    "    loftq_config = None, # LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d7cca53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200026/200026 [00:01<00:00, 160184.13 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9062/9062 [00:00<00:00, 125440.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "\n",
    "    instruction = 'Perform Diagnosis'\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for inputx, output in zip(inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, inputx, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "pass\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca94e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset['train'],\n",
    "    #eval_dataset = dataset['validation'],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 1024,\n",
    "    tokenizer = tokenizer,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 16,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 100,\n",
    "        num_train_epochs = 1,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        output_dir = \"gemma_outputs\",\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        optim = \"adamw_8bit\",\n",
    "        seed = 3411,\n",
    "    ),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9386c2f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphi_lora_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained_merged(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphi_merged_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer, save_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerged_16bit\u001b[39m\u001b[38;5;124m\"\u001b[39m,)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"phi_lora_model\")\n",
    "model.save_pretrained_merged(\"phi_merged_model\", tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64787a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model,tokenizer, text, max_length):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=300)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c35a3a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_unsloth_model_and_tokenizer(model_path):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_path,\n",
    "        max_seq_length = 1024,\n",
    "        dtype = None,\n",
    "        load_in_4bit = True,\n",
    "        local_files_only = True,\n",
    "    )\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "982227a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def generate_predictions(test_file, model, tokenizer, output_file, max_new_tokens=200):\n",
    "    \"\"\"\n",
    "    Generate predictions for test samples and save to a JSONL file.\n",
    "\n",
    "    Args:\n",
    "        test_file (str): Path to the test samples JSON file containing an array of objects.\n",
    "        model: Loaded Unsloth model.\n",
    "        tokenizer: Tokenizer for the model.\n",
    "        output_file (str): Path to save the model predictions JSONL file.\n",
    "        max_new_tokens (int): Maximum number of new tokens to generate.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "\n",
    "    # Load the entire JSON array from the file\n",
    "    with open(test_file, 'r') as f:\n",
    "        data = json.load(f)  # Parse JSON as an array of objects\n",
    "\n",
    "    for item in data:\n",
    "        try:\n",
    "            instruction = item.get(\"instruction\", \"Perform Diagnosis\")\n",
    "            human_input = item[\"input\"]\n",
    "            #print(f\"Processing input: {human_input}\")\n",
    "\n",
    "            # Format input for the model using the Alpaca-style template\n",
    "            prompt = alpaca_prompt.format(instruction, human_input, \"\")\n",
    "\n",
    "            # Tokenize the input\n",
    "            inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "            # Generate predictions\n",
    "            outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, use_cache=True)\n",
    "            prediction_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "            print(prediction_text)\n",
    "\n",
    "            # Save the prediction\n",
    "            predictions.append({\n",
    "                \"instruction\": instruction,\n",
    "                \"input\": human_input,\n",
    "                \"output\": prediction_text.strip()\n",
    "            })\n",
    "        except KeyError as e:\n",
    "            print(f\"Skipping item due to missing key: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Write all predictions to the output file in JSONL format\n",
    "    with open(output_file, 'w') as f:\n",
    "        for prediction in predictions:\n",
    "            f.write(json.dumps(prediction) + '\\n')\n",
    "\n",
    "    print(f\"Predictions saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4811f192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.7: Fast Llama patching. Transformers = 4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA H100 80GB HBM3. Max memory: 79.097 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1+cu124. CUDA = 9.0. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.dev941. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model_path = \"phi_outputs/checkpoint-3125\"\n",
    "test_file = \"sampled_test_combined_data.json\"  # Test samples JSONL file\n",
    "model_output_file = \"model_predictions.json\"  # Output predictions JSONL file\n",
    "model, tokenizer = load_unsloth_model_and_tokenizer(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887c9ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "# Generate predictions\n",
    "generate_predictions(test_file, model, tokenizer, model_output_file)\n",
    "print(f\"Predictions saved to {model_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9646f72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPerform Diagnosis in the format: Differential Diagnosis is: a,b,c... and Disease can be X\\n\\n### Input:\\nAge: 70, Sex: F. History:Have you been in contact with a person with similar symptoms in the past 2 weeks?Yes; Have you traveled out of the country in the last 4 weeks?: No. Symptoms: Do you have a cough?Yes; Have you had significantly increased sweating?Yes; Do you have pain somewhere, related to your reason for consulting?Yes; Characterize your pain:: sensitive; Characterize your pain:: heavy; Do you feel pain somewhere?: top of the head; Do you feel pain somewhere?: forehead; Do you feel pain somewhere?: cheek(R); Do you feel pain somewhere?: cheek(L); Do you feel pain somewhere?: occiput; How intense is the pain?: 7; Does the pain radiate to another location?: nowhere; How precisely is the pain located?: 3; How fast did the pain appear?: 0; Do you have a fever (either felt or measured with a thermometer)?Yes; Do you have a sore throat?Yes; Do you have diffuse (widespread) muscle pain?Yes; Do you have nasal congestion or a clear runny nose?Yes; Do you have a cough?Yes.\\n\\n### Response:\\nDifferential Diagnosis is: URTI, Influenza, HIV (initial infection), Bronchitis, Tuberculosis, Pneumonia, Chronic rhinosinusitis, Chagas, Acute rhinosinusitis and the Disease can be URTI <|endoftext|>']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "# Generate predictions\n",
    "inp = \"Age: 70, Sex: F. History:Have you been in contact with a person with similar symptoms in the past 2 weeks?Yes; Have you traveled out of the country in the last 4 weeks?: No. Symptoms: Do you have a cough?Yes; Have you had significantly increased sweating?Yes; Do you have pain somewhere, related to your reason for consulting?Yes; Characterize your pain:: sensitive; Characterize your pain:: heavy; Do you feel pain somewhere?: top of the head; Do you feel pain somewhere?: forehead; Do you feel pain somewhere?: cheek(R); Do you feel pain somewhere?: cheek(L); Do you feel pain somewhere?: occiput; How intense is the pain?: 7; Does the pain radiate to another location?: nowhere; How precisely is the pain located?: 3; How fast did the pain appear?: 0; Do you have a fever (either felt or measured with a thermometer)?Yes; Do you have a sore throat?Yes; Do you have diffuse (widespread) muscle pain?Yes; Do you have nasal congestion or a clear runny nose?Yes; Do you have a cough?Yes.\"\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Perform Diagnosis in the format: Differential Diagnosis is: a,b,c... and Disease can be X\",\n",
    "        inp, # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 512, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1358d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.69%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def extract_disease(output_text):\n",
    "    \"\"\"\n",
    "    Extracts the disease mentioned after 'Disease can be' in the output text.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"Disease can be (.*)\", output_text)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "def calculate_accuracy_from_files(test_file, prediction_file):\n",
    "    \"\"\"\n",
    "    Calculates accuracy by comparing the test outputs to prediction responses from two JSON files.\n",
    "\n",
    "    Args:\n",
    "        test_file (str): Path to the test data JSON file.\n",
    "        prediction_file (str): Path to the prediction data JSON file.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy percentage.\n",
    "    \"\"\"\n",
    "    # Load test data\n",
    "    with open(test_file, \"r\") as test_f:\n",
    "        test_data = json.load(test_f)\n",
    "\n",
    "    # Load prediction data\n",
    "    with open(prediction_file, \"r\") as pred_f:\n",
    "        prediction_data = json.load(pred_f)\n",
    "\n",
    "    if len(test_data) != len(prediction_data):\n",
    "        raise ValueError(\"Test data and prediction data must have the same number of entries.\")\n",
    "\n",
    "    correct_count = 0\n",
    "\n",
    "\n",
    "    for test_entry, pred_entry in zip(test_data, prediction_data):\n",
    "        # Extract diseases\n",
    "        test_disease = extract_disease(test_entry[\"output\"])\n",
    "        pred_disease = extract_disease(pred_entry[\"output\"])\n",
    "\n",
    "        # Compare diseases\n",
    "        if test_disease and pred_disease and test_disease == pred_disease:\n",
    "            correct_count += 1\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = (correct_count / len(test_data)) * 100\n",
    "    return accuracy\n",
    "\n",
    "# Example usage\n",
    "test_file = \"sampled_test_combined_data.json\"  # Replace with the path to your test JSON file\n",
    "prediction_file = \"formatted_data.json\"  # Replace with the path to your prediction JSON file\n",
    "\n",
    "accuracy = calculate_accuracy_from_files(test_file, prediction_file)\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3995eba8-2779-4460-9d1e-08bd01e756f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision_differential': 0.9176261585108249, 'recall_differential': 0.9267435226579821, 'f1_differential': 0.9121278857091725, 'rouge_differential': 0.8773032723351539}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def extract_differential_diagnoses(output_text):\n",
    "    \"\"\"\n",
    "    Extracts the differential diagnoses from the output text.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"Differential diagnosis is: (.*?) and the Disease can be\", output_text)\n",
    "    if match:\n",
    "        diagnoses = match.group(1).strip()\n",
    "        return set(diagnoses.split(\", \"))  # Split diagnoses into a set\n",
    "    return set()\n",
    "\n",
    "def calculate_metrics(test_file, prediction_file):\n",
    "    \"\"\"\n",
    "    Calculates metrics like precision, recall, F1-score, and ROUGE for differential diagnoses.\n",
    "\n",
    "    Args:\n",
    "        test_file (str): Path to the test data JSON file.\n",
    "        prediction_file (str): Path to the prediction data JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing accuracy, precision, recall, F1-score, and ROUGE metrics.\n",
    "    \"\"\"\n",
    "    # Load test data\n",
    "    with open(test_file, \"r\") as test_f:\n",
    "        test_data = json.load(test_f)\n",
    "\n",
    "    # Load prediction data\n",
    "    with open(prediction_file, \"r\") as pred_f:\n",
    "        prediction_data = json.load(pred_f)\n",
    "\n",
    "    if len(test_data) != len(prediction_data):\n",
    "        raise ValueError(\"Test data and prediction data must have the same number of entries.\")\n",
    "\n",
    "    precision_list, recall_list, f1_list, rouge_list = [], [], [], []\n",
    "    correct_count = 0\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "    for test_entry, pred_entry in zip(test_data, prediction_data):\n",
    "        # Extract differential diagnoses\n",
    "        test_differential = extract_differential_diagnoses(test_entry[\"output\"])\n",
    "        pred_differential = extract_differential_diagnoses(pred_entry[\"output\"])\n",
    "\n",
    "        # Calculate precision, recall, and F1\n",
    "        tp = len(test_differential & pred_differential)  # True Positives\n",
    "        fp = len(pred_differential - test_differential)  # False Positives\n",
    "        fn = len(test_differential - pred_differential)  # False Negatives\n",
    "\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "\n",
    "        # Calculate ROUGE-L\n",
    "        test_diagnoses_str = \", \".join(test_differential)\n",
    "        pred_diagnoses_str = \", \".join(pred_differential)\n",
    "        rouge = scorer.score(test_diagnoses_str, pred_diagnoses_str)['rougeL'].fmeasure\n",
    "        rouge_list.append(rouge)\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_precision = sum(precision_list) / len(precision_list)\n",
    "    avg_recall = sum(recall_list) / len(recall_list)\n",
    "    avg_f1 = sum(f1_list) / len(f1_list)\n",
    "    avg_rouge = sum(rouge_list) / len(rouge_list)\n",
    "\n",
    "    return {\n",
    "        \"precision_differential\": avg_precision,\n",
    "        \"recall_differential\": avg_recall,\n",
    "        \"f1_differential\": avg_f1,\n",
    "        \"rouge_differential\": avg_rouge,\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "test_file = \"sampled_test_combined_data.json\"  # Replace with the path to your test JSON file\n",
    "prediction_file = \"formatted_data.json\"  # Replace with the path to your prediction JSON file\n",
    "\n",
    "metrics = calculate_metrics(test_file, prediction_file)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c81c3ee-953a-4526-9110-d44007ee808a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
