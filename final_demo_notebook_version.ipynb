{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39a98807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "~/scratch/hf-cache\n"
     ]
    }
   ],
   "source": [
    "SHOULD_TRAIN = False\n",
    "#SHOULD_TRAIN = True\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "\n",
    "# disable Weights and Biases\n",
    "os.environ['WANDB_DISABLED']=\"true\"\n",
    "os.environ[\"HF_HOME\"] = \"~/scratch/hf-cache\"\n",
    "token=\"\"\n",
    "print(os.environ['WANDB_DISABLED'])  # Should output \"true\"\n",
    "print(os.environ['HF_HOME'])  # Should output \"~/scratch/hf-cache\"\n",
    "\n",
    "#output_file = open('logger_tests.log', 'w')\n",
    "#sys.stdout = output_file\n",
    "#sys.stderr = output_file\n",
    "\n",
    "LANG_TOKEN_MAPPING = {\n",
    "    'hi': '',\n",
    "    'en': ''\n",
    "}\n",
    "max_seq_len = 25\n",
    "last_print_time = time.time()\n",
    "model_path = \"./models/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "944c0409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "~/scratch/hf-cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to ./nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to ./nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to ./nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to ./nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to ./nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to ./nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to ./nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to ./nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to ./nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to ./nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to ./nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to ./nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to ./nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to ./nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to ./nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to ./nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to ./nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to ./nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to ./nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to ./nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to ./nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to ./nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to ./nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to ./nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to ./nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to ./nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to ./nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to ./nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to ./nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to ./nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to ./nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to ./nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to ./nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to ./nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to ./nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to ./nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to ./nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to ./nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to ./nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to ./nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to ./nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to ./nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to ./nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to ./nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to ./nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to ./nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to ./nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to ./nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to ./nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to ./nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to ./nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to ./nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to ./nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to ./nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to ./nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to ./nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to ./nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to ./nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to ./nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to ./nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to ./nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to ./nltk_data...\n",
      "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to ./nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to ./nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to ./nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to ./nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to ./nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to ./nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to ./nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to ./nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to ./nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to ./nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to ./nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to ./nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to ./nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to ./nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to ./nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to ./nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to ./nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to ./nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets_json to ./nltk_data...\n",
      "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to ./nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to ./nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to ./nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to ./nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to ./nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to ./nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to ./nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to ./nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to ./nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to ./nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to ./nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to ./nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to ./nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to ./nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to ./nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to ./nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to ./nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to ./nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to ./nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to ./nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Unsloth 2024.11.11 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules and functions from translate_codebase.py\n",
    "\n",
    "from translate_codebase import (\n",
    "    # Environment variables and settings\n",
    "    # Generic Helper functions\n",
    "    clock_begin,\n",
    "    clock_end,\n",
    "    read_token_and_login,\n",
    "    print_gpu_utilization,\n",
    "    clear_cuda_cache,\n",
    "    # Metrics Functions\n",
    "    evaluate_translations_bertscore,\n",
    "    evaluate_translations_rouge,\n",
    "    evaluate_translations_bleu,\n",
    "    evaluate_translations_meteor,\n",
    "    # Common Data Preprocessing Functions\n",
    "    filter_sentences,\n",
    "    get_reduced_dataset,\n",
    "    prepare_test_data,\n",
    "    perform_translation_testing,\n",
    "    # MBART related functions\n",
    "    get_pretrained_mbart_large_50_many_to_many_mmt,\n",
    "    preprocess_function_mbart,\n",
    "    prepare_model_for_training_mbart,\n",
    "    fine_tune_and_save_mbart,\n",
    "    load_fine_tuned_model_mbart,\n",
    "    translate_text_mbart,\n",
    "    # MT5 related functions\n",
    "    get_pretrained_mt5_small,\n",
    "    config_mt5_small,\n",
    "    encode_input_str_mt5_small,\n",
    "    encode_target_str_mt5_small,\n",
    "    process_translation_list_mt5_small,\n",
    "    format_translation_data_mt5_small,\n",
    "    transform_batch_mt5_small,\n",
    "    fine_tune_and_save_model_mt5_small,\n",
    "    eval_model_mt5_small,\n",
    "    save_fine_tuned_model_mt5_small,\n",
    "    load_fine_tuned_model_mt5_small,\n",
    "    translate_text_mt5_small\n",
    ")\n",
    "\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "from team_files.akshay import getDiagnosisLlama\n",
    "from team_files.aditya import load_unsloth_model_and_tokenizer_phi, generate_diagnosis_phi\n",
    "from team_files.archit import load_model_mis, generate_text_mis, inference_mis\n",
    "from team_files.ensemble import ensemble_responses\n",
    "\n",
    "#from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2c6a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mbart_eng_hin_huggingface(force_download=False):\n",
    "\n",
    "    model_path = \"sathvikaithalkp456/mbart_fine_tuned_eng_hin\"\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(model_path, force_download = force_download)\n",
    "    model = MBartForConditionalGeneration.from_pretrained(model_path, force_download = force_download)\n",
    "    tokenizer.src_lang = \"en_XX\"\n",
    "    tokenizer.tgt_lang = \"hi_IN\"\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_mbart_hin_eng_huggingface(force_download=False):\n",
    "    revision = \"master\"\n",
    "    model_path = \"sathvikaithalkp456/mbart_fine_tuned_hin_eng\"\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(model_path, force_download = force_download, revision=revision)\n",
    "    model = MBartForConditionalGeneration.from_pretrained(model_path, force_download = force_download, revision=revision)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_mt5_small_eng_hin_huggingface(force_download=False):\n",
    "\n",
    "    revision = \"master\"\n",
    "    model_path = 'sathvikaithalkp456/mt5_small_fine_tuned_eng_hindi'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, force_download = force_download, revision=revision)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path, force_download = force_download, revision=revision)\n",
    "    model = model.cuda()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_mt5_small_hin_eng_huggingface(force_download=False):\n",
    "\n",
    "    revision = \"master\"\n",
    "    model_path = 'sathvikaithalkp456/mt5_small_fine_tuned_hindi_eng'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, force_download = force_download, revision=revision)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path, force_download = force_download, revision=revision)\n",
    "    model = model.cuda()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def translate_text_generic(model, tokenizer, input_text, src_lang, tgt_lang, model_type=\"mbart\"):\n",
    "    \"\"\"\n",
    "    Translates a given text from the source language to the target language.\n",
    "\n",
    "    Args:\n",
    "        model: The translation model.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "        input_text: The input text to be translated.\n",
    "        src_lang: The source language code.\n",
    "        tgt_lang: The target language code.\n",
    "        model_type: The type of model (\"mbart\" or \"mt5\").\n",
    "\n",
    "    Returns:\n",
    "        str: The translated text.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    if model_type == \"mbart\":\n",
    "        # Tokenize the input text with padding and truncation\n",
    "        tokenizer.src_lang = src_lang\n",
    "        encoded_input = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "        # Generate translation\n",
    "        generated_tokens = model.generate(\n",
    "            **encoded_input,\n",
    "            forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang]\n",
    "        )\n",
    "\n",
    "        # Decode the generated tokens\n",
    "        translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    else:  # mt5\n",
    "        # Encode the input text with padding and truncation\n",
    "        input_ids = encode_input_str_mt5_small(\n",
    "            text=input_text,\n",
    "            target_lang=tgt_lang,\n",
    "            tokenizer=tokenizer,\n",
    "            seq_len=model.config.max_length,\n",
    "            lang_token_map=LANG_TOKEN_MAPPING\n",
    "        )\n",
    "        input_ids = input_ids.unsqueeze(0).to(device)\n",
    "\n",
    "        # Generate the translation\n",
    "        output_ids = model.generate(input_ids, num_beams=5, max_length=model.config.max_length, early_stopping=True)\n",
    "\n",
    "        # Decode the generated tokens\n",
    "        translated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    #print(\"Translated text:\", translated_text)\n",
    "    return translated_text\n",
    "\n",
    "\n",
    "def translate_sentences_generic(input_lists, output_file, src_lang, tgt_lang, model_type=\"mbart\", direction=\"eng_hin\"):\n",
    "    \"\"\"\n",
    "    Translates each sentence in the input lists to the target language and saves the translated sentences to a file.\n",
    "\n",
    "    Args:\n",
    "        input_lists (list): A list of lists, where each inner list contains sentences to be translated.\n",
    "        output_file (str): The path to the output file where translated sentences will be saved.\n",
    "        src_lang (str): The source language code.\n",
    "        tgt_lang (str): The target language code.\n",
    "        model_type (str): The type of model to use for translation (\"mbart\" or \"mt5\").\n",
    "        direction (str): The direction of translation (\"eng_hin\" or \"hin_eng\").\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if model_type == \"mbart\":\n",
    "        if direction == \"eng_hin\":\n",
    "            model, tokenizer = get_mbart_eng_hin_huggingface()\n",
    "        else:  # hin_eng\n",
    "            model, tokenizer = get_mbart_hin_eng_huggingface()\n",
    "    else:  # mt5\n",
    "        if direction == \"eng_hin\":\n",
    "            model, tokenizer = get_mt5_small_eng_hin_huggingface()\n",
    "        else:  # hin_eng\n",
    "            model, tokenizer = get_mt5_small_hin_eng_huggingface()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    translated_data = []\n",
    "\n",
    "    for input_list in input_lists:\n",
    "        translated_list = []\n",
    "        for sentence in input_list:\n",
    "            translated_sentence = translate_text_generic(model, tokenizer, sentence, src_lang, tgt_lang, model_type=model_type)\n",
    "            translated_list.append(translated_sentence)\n",
    "        translated_data.append(translated_list)\n",
    "\n",
    "    # Save the translated sentences to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(translated_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def parse_input(data):\n",
    "    \"\"\"\n",
    "    Parses the input data, replaces 'Y' with 'Yes' and 'N' with 'No', and returns it as a list of formatted inputs.\n",
    "\n",
    "    Args:\n",
    "        data (str): The input data in JSON format.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of formatted inputs.\n",
    "    \"\"\"\n",
    "    parsed_data = json.loads(data)\n",
    "    formatted_inputs = []\n",
    "\n",
    "    for item in parsed_data:\n",
    "        input_text = item['input']\n",
    "        # Replace ' Y ' with ' Yes ' and ' N ' with ' No '\n",
    "        input_text = input_text.replace(' Y ', ' Yes ').replace(' N ', ' No ')\n",
    "        # Split the input text into individual lines based on delimiters\n",
    "        lines = re.split(r'[;.]', input_text)\n",
    "        # Strip leading and trailing whitespace from each line and filter out empty lines\n",
    "        lines = [line.strip() for line in lines if line.strip()]\n",
    "        formatted_inputs.append(lines)\n",
    "\n",
    "    return formatted_inputs\n",
    "\n",
    "def generate_input():\n",
    "\n",
    "    with open('sample_data.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    data = json.dumps(data)\n",
    "\n",
    "    parsed_inputs = parse_input(data)\n",
    "    print(parsed_inputs)\n",
    "\n",
    "    output_file = \"translated_sentences_hindi.json\"\n",
    "    translate_sentences_generic(parsed_inputs, output_file, src_lang=\"en_XX\", tgt_lang=\"hi_IN\", model_type=\"mbart\", direction=\"eng_hin\")\n",
    "\n",
    "    print(\"Input generated!!\")\n",
    "\n",
    "def get_inputs():\n",
    "    with open('translated_sentences_hindi.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "    print(len(data))\n",
    "    #print(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Function to translate a sample input back to English\n",
    "def translate_sample_to_english(sample_input):\n",
    "    # Wrap the sample input in a list of lists to use with translate_sentences_generic\n",
    "    input_lists = [sample_input]\n",
    "    output_file = \"temp_file.json\"\n",
    "    \n",
    "    # Translate the sample input back to English\n",
    "    translate_sentences_generic(input_lists, output_file, src_lang=\"hi_IN\", tgt_lang=\"en_XX\", model_type=\"mbart\", direction=\"hin_eng\")\n",
    "    \n",
    "    # Load the translated text from the output file\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        translated_data = json.load(f)\n",
    "    \n",
    "    # Extract the translated text\n",
    "    print(\"translated data: \", translated_data)\n",
    "    #translated_text = translated_data\n",
    "    #print(\"Translated text:\", translated_text)\n",
    "    return translated_data\n",
    "\n",
    "\n",
    "def translate_single_sample_to_hindi(diagnosis):\n",
    "    # Wrap the sample input in a list of lists to use with translate_sentences_generic\n",
    "    input_lists = [[diagnosis]]\n",
    "    output_file = \"temp_file_2.json\"\n",
    "    \n",
    "    # Translate the sample input back to English\n",
    "    translate_sentences_generic(input_lists, output_file, src_lang=\"en_XX\", tgt_lang=\"hi_IN\", model_type=\"mbart\", direction=\"eng_hin\")\n",
    "    \n",
    "    # Load the translated text from the output file\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        translated_data = json.load(f)\n",
    "    \n",
    "    # Extract the translated text\n",
    "    print(\"translated data: \", translated_data)\n",
    "    #translated_text = translated_data\n",
    "    #print(\"Translated text:\", translated_text)\n",
    "    return translated_data\n",
    "\n",
    "\n",
    "# Function to translate a sample input back to English\n",
    "def translate_sample_to_hindi(diagnosis,diagnosis_unsloth, diagnosis_mistral):\n",
    "    # Wrap the sample input in a list of lists to use with translate_sentences_generic\n",
    "    input_lists = [[diagnosis],[diagnosis_unsloth], [diagnosis_mistral]]\n",
    "    output_file = \"temp_file_2.json\"\n",
    "    \n",
    "    # Translate the sample input back to English\n",
    "    translate_sentences_generic(input_lists, output_file, src_lang=\"en_XX\", tgt_lang=\"hi_IN\", model_type=\"mbart\", direction=\"eng_hin\")\n",
    "    \n",
    "    # Load the translated text from the output file\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        translated_data = json.load(f)\n",
    "    \n",
    "    # Extract the translated text\n",
    "    print(\"translated data: \", translated_data)\n",
    "    #translated_text = translated_data\n",
    "    #print(\"Translated text:\", translated_text)\n",
    "    return translated_data\n",
    "\n",
    "\n",
    "def process_translated_text_and_get_diagnosis_llama(translated_text):\n",
    "    \"\"\"\n",
    "    Processes the translated text, joins the sentences, and gets the diagnosis from the Medical LLM (Llama).\n",
    "\n",
    "    Args:\n",
    "        translated_text (list): A list of translated sentences.\n",
    "\n",
    "    Returns:\n",
    "        str: The diagnosis from the Medical LLM (Llama).\n",
    "    \"\"\"\n",
    "    # Flatten the list of lists into a single list of strings\n",
    "    flattened_text = [sentence for sublist in translated_text for sentence in sublist]\n",
    "    \n",
    "    # Join the sentences in the translated text\n",
    "    combined_text = ' '.join(flattened_text)\n",
    "    print(f\"Combined text: {combined_text}\")\n",
    "\n",
    "    # Pass the combined text to the Medical LLM and get the diagnosis\n",
    "    diagnosis, diagnosis_json = getDiagnosisLlama(combined_text)\n",
    "    return diagnosis, diagnosis_json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_translated_text_and_get_diagnosis_unsloth(translated_text):\n",
    "    \"\"\"\n",
    "    Processes the translated text, joins the sentences, and gets the diagnosis from the Medical LLM (Unsloth).\n",
    "\n",
    "    Args:\n",
    "        translated_text (list): A list of translated sentences.\n",
    "\n",
    "    Returns:\n",
    "        str: The diagnosis from the Medical LLM (Unsloth).\n",
    "    \"\"\"\n",
    "    # Flatten the list of lists into a single list of strings\n",
    "    flattened_text = [sentence for sublist in translated_text for sentence in sublist]\n",
    "    \n",
    "    # Join the sentences in the translated text\n",
    "    combined_text = ' '.join(flattened_text)\n",
    "    print(f\"Combined text: {combined_text}\")\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    model_path = \"Buddy1421/medical_diagnosis_phi_3-5\"\n",
    "    model, tokenizer = load_unsloth_model_and_tokenizer_phi(model_path, use_safetensors=True)\n",
    "\n",
    "    # Pass the combined text to the Medical LLM and get the diagnosis\n",
    "    diagnosis, diagnosis_json= generate_diagnosis_phi(model, tokenizer, combined_text)\n",
    "    return diagnosis,diagnosis_json\n",
    "\n",
    "\n",
    "def process_translated_text_and_get_diagnosis_mistral(translated_text):\n",
    "    \"\"\"\n",
    "    Processes the translated text, joins the sentences, and gets the diagnosis from the Medical LLM (Mistral).\n",
    "\n",
    "    Args:\n",
    "        translated_text (list): A list of translated sentences.\n",
    "\n",
    "    Returns:\n",
    "        str: The diagnosis from the Medical LLM (Mistral).\n",
    "    \"\"\"\n",
    "    # Flatten the list of lists into a single list of strings\n",
    "    flattened_text = [sentence for sublist in translated_text for sentence in sublist]\n",
    "    \n",
    "    # Join the sentences in the translated text\n",
    "    combined_text = ' '.join(flattened_text)\n",
    "    print(f\"Combined text: {combined_text}\")\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    #model, tokenizer = load_model_mis()\n",
    "\n",
    "    # Pass the combined text to the Medical LLM and get the diagnosis\n",
    "    full_response , diagnosis_json = inference_mis(combined_text, max_length=512)\n",
    "    return full_response, diagnosis_json\n",
    "\n",
    "def format_for_ensemble(mistral_json, llama_json, phi_json):\n",
    "    outputs = {\n",
    "        'mistral' :mistral_json,\n",
    "        'llama': llama_json,\n",
    "        'phi': phi_json,\n",
    "    }\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "621e7431",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"starting demo\")\n",
    "#print(\"generating inputs\")\n",
    "#generate_input()\n",
    "print(\"getting inputs\")\n",
    "inputs = get_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "573e99ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = 4\n",
    "#print(\"Choosing 1 random input : \",example, \" from 25 inputs\")\n",
    "test_input = inputs[example]\n",
    "print(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "218e12df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Translating to English\")\n",
    "translated_text = translate_sample_to_english(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa218341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "print(\"Calling MEDICAL LLM (Llama)\")\n",
    "diagnosis,llama_diagnosis_json = process_translated_text_and_get_diagnosis_llama(translated_text)\n",
    "print(\"Diagnosis:\", diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "156e8304",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calling MEDICAL LLM (PHI)\")\n",
    "diagnosis_unsloth, phi_json = process_translated_text_and_get_diagnosis_unsloth(translated_text)\n",
    "print(\"Diagnosis (Unsloth):\", diagnosis_unsloth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aed9626",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calling MEDICAL LLM (Mistral)\")\n",
    "diagnosis_mistral, mistral_diagnosis_json  = process_translated_text_and_get_diagnosis_mistral(translated_text)\n",
    "print(\"Diagnosis (Mistral):\", diagnosis_mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43b06826",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs = format_for_ensemble(mistral_diagnosis_json, llama_diagnosis_json, phi_json)\n",
    "print(\"Final English Response: \")\n",
    "ensembled = ensemble_responses(model_outputs)\n",
    "print(ensembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73eca285",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_final = translate_single_sample_to_hindi(ensembled)\n",
    "print(f'Final Hindi Response {translated_final}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c679deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4954bcc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8835f925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddba9ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
