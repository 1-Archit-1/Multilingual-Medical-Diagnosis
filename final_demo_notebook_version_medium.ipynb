{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39a98807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "~/scratch/hf-cache\n"
     ]
    }
   ],
   "source": [
    "SHOULD_TRAIN = False\n",
    "#SHOULD_TRAIN = True\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "\n",
    "# disable Weights and Biases\n",
    "os.environ['WANDB_DISABLED']=\"true\"\n",
    "os.environ[\"HF_HOME\"] = \"~/scratch/hf-cache\"\n",
    "token=\"\"\n",
    "print(os.environ['WANDB_DISABLED'])  # Should output \"true\"\n",
    "print(os.environ['HF_HOME'])  # Should output \"~/scratch/hf-cache\"\n",
    "\n",
    "output_file = open('logger_tests.log', 'w')\n",
    "sys.stdout = output_file\n",
    "sys.stderr = output_file\n",
    "\n",
    "LANG_TOKEN_MAPPING = {\n",
    "    'hi': '',\n",
    "    'en': ''\n",
    "}\n",
    "max_seq_len = 25\n",
    "last_print_time = time.time()\n",
    "model_path = \"./models/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "944c0409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules and functions from translate_codebase.py\n",
    "\n",
    "from translate_codebase import (\n",
    "    # Environment variables and settings\n",
    "    # Generic Helper functions\n",
    "    clock_begin,\n",
    "    clock_end,\n",
    "    read_token_and_login,\n",
    "    print_gpu_utilization,\n",
    "    clear_cuda_cache,\n",
    "    # Metrics Functions\n",
    "    evaluate_translations_bertscore,\n",
    "    evaluate_translations_rouge,\n",
    "    evaluate_translations_bleu,\n",
    "    evaluate_translations_meteor,\n",
    "    # Common Data Preprocessing Functions\n",
    "    filter_sentences,\n",
    "    get_reduced_dataset,\n",
    "    prepare_test_data,\n",
    "    perform_translation_testing,\n",
    "    # MBART related functions\n",
    "    get_pretrained_mbart_large_50_many_to_many_mmt,\n",
    "    preprocess_function_mbart,\n",
    "    prepare_model_for_training_mbart,\n",
    "    fine_tune_and_save_mbart,\n",
    "    load_fine_tuned_model_mbart,\n",
    "    translate_text_mbart,\n",
    "    # MT5 related functions\n",
    "    get_pretrained_mt5_small,\n",
    "    config_mt5_small,\n",
    "    encode_input_str_mt5_small,\n",
    "    encode_target_str_mt5_small,\n",
    "    process_translation_list_mt5_small,\n",
    "    format_translation_data_mt5_small,\n",
    "    transform_batch_mt5_small,\n",
    "    fine_tune_and_save_model_mt5_small,\n",
    "    eval_model_mt5_small,\n",
    "    save_fine_tuned_model_mt5_small,\n",
    "    load_fine_tuned_model_mt5_small,\n",
    "    translate_text_mt5_small\n",
    ")\n",
    "\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "from team_files.akshay import getDiagnosisLlama\n",
    "from team_files.aditya import load_unsloth_model_and_tokenizer_phi, generate_diagnosis_phi\n",
    "from team_files.archit import load_model_mis, generate_text_mis, inference_mis\n",
    "from team_files.ensemble import ensemble_responses\n",
    "\n",
    "#from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f2c6a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mbart_eng_hin_huggingface(force_download=True):\n",
    "\n",
    "    model_path = \"sathvikaithalkp456/mbart_fine_tuned_eng_hin\"\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(model_path, force_download = force_download)\n",
    "    model = MBartForConditionalGeneration.from_pretrained(model_path, force_download = force_download)\n",
    "    tokenizer.src_lang = \"en_XX\"\n",
    "    tokenizer.tgt_lang = \"hi_IN\"\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_mbart_hin_eng_huggingface(force_download=True):\n",
    "    revision = \"master\"\n",
    "    model_path = \"sathvikaithalkp456/mbart_fine_tuned_hin_eng\"\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(model_path, force_download = force_download, revision=revision)\n",
    "    model = MBartForConditionalGeneration.from_pretrained(model_path, force_download = force_download, revision=revision)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_mt5_small_eng_hin_huggingface(force_download=True):\n",
    "\n",
    "    revision = \"master\"\n",
    "    model_path = 'sathvikaithalkp456/mbart_fine_tuned_eng_hin'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, force_download = force_download, revision=revision)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path, force_download = force_download, revision=revision)\n",
    "    model = model.cuda()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_mt5_small_hin_eng_huggingface(force_download=True):\n",
    "\n",
    "    revision = \"master\"\n",
    "    model_path = 'sathvikaithalkp456/mbart_fine_tuned_hin_eng'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, force_download = force_download, revision=revision)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path, force_download = force_download, revision=revision)\n",
    "    model = model.cuda()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def translate_text_generic(model, tokenizer, input_text, src_lang, tgt_lang, model_type=\"mbart\"):\n",
    "    \"\"\"\n",
    "    Translates a given text from the source language to the target language.\n",
    "\n",
    "    Args:\n",
    "        model: The translation model.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "        input_text: The input text to be translated.\n",
    "        src_lang: The source language code.\n",
    "        tgt_lang: The target language code.\n",
    "        model_type: The type of model (\"mbart\" or \"mt5\").\n",
    "\n",
    "    Returns:\n",
    "        str: The translated text.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    if model_type == \"mbart\":\n",
    "        # Tokenize the input text with padding and truncation\n",
    "        tokenizer.src_lang = src_lang\n",
    "        encoded_input = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "        # Generate translation\n",
    "        generated_tokens = model.generate(\n",
    "            **encoded_input,\n",
    "            forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang]\n",
    "        )\n",
    "\n",
    "        # Decode the generated tokens\n",
    "        translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    else:  # mt5\n",
    "        # Encode the input text with padding and truncation\n",
    "        input_ids = encode_input_str_mt5_small(\n",
    "            text=input_text,\n",
    "            target_lang=tgt_lang,\n",
    "            tokenizer=tokenizer,\n",
    "            seq_len=model.config.max_length,\n",
    "            lang_token_map=LANG_TOKEN_MAPPING\n",
    "        )\n",
    "        input_ids = input_ids.unsqueeze(0).to(device)\n",
    "\n",
    "        # Generate the translation\n",
    "        output_ids = model.generate(input_ids, num_beams=5, max_length=model.config.max_length, early_stopping=True)\n",
    "\n",
    "        # Decode the generated tokens\n",
    "        translated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    #print(\"Translated text:\", translated_text)\n",
    "    return translated_text\n",
    "\n",
    "\n",
    "def translate_sentences_generic(input_lists, output_file, src_lang, tgt_lang, model_type=\"mbart\", direction=\"eng_hin\"):\n",
    "    \"\"\"\n",
    "    Translates each sentence in the input lists to the target language and saves the translated sentences to a file.\n",
    "\n",
    "    Args:\n",
    "        input_lists (list): A list of lists, where each inner list contains sentences to be translated.\n",
    "        output_file (str): The path to the output file where translated sentences will be saved.\n",
    "        src_lang (str): The source language code.\n",
    "        tgt_lang (str): The target language code.\n",
    "        model_type (str): The type of model to use for translation (\"mbart\" or \"mt5\").\n",
    "        direction (str): The direction of translation (\"eng_hin\" or \"hin_eng\").\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if model_type == \"mbart\":\n",
    "        if direction == \"eng_hin\":\n",
    "            model, tokenizer = get_mbart_eng_hin_huggingface()\n",
    "        else:  # hin_eng\n",
    "            model, tokenizer = get_mbart_hin_eng_huggingface()\n",
    "    else:  # mt5\n",
    "        if direction == \"eng_hin\":\n",
    "            model, tokenizer = get_mt5_small_eng_hin_huggingface()\n",
    "        else:  # hin_eng\n",
    "            model, tokenizer = get_mt5_small_hin_eng_huggingface()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    translated_data = []\n",
    "\n",
    "    for input_list in input_lists:\n",
    "        translated_list = []\n",
    "        for sentence in input_list:\n",
    "            translated_sentence = translate_text_generic(model, tokenizer, sentence, src_lang, tgt_lang, model_type=model_type)\n",
    "            translated_list.append(translated_sentence)\n",
    "        translated_data.append(translated_list)\n",
    "\n",
    "    # Save the translated sentences to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(translated_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def parse_input(data):\n",
    "    \"\"\"\n",
    "    Parses the input data, replaces 'Y' with 'Yes' and 'N' with 'No', and returns it as a list of formatted inputs.\n",
    "\n",
    "    Args:\n",
    "        data (str): The input data in JSON format.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of formatted inputs.\n",
    "    \"\"\"\n",
    "    parsed_data = json.loads(data)\n",
    "    formatted_inputs = []\n",
    "\n",
    "    for item in parsed_data:\n",
    "        input_text = item['input']\n",
    "        # Replace ' Y ' with ' Yes ' and ' N ' with ' No '\n",
    "        input_text = input_text.replace(' Y ', ' Yes ').replace(' N ', ' No ')\n",
    "        # Split the input text into individual lines based on delimiters\n",
    "        lines = re.split(r'[;.]', input_text)\n",
    "        # Strip leading and trailing whitespace from each line and filter out empty lines\n",
    "        lines = [line.strip() for line in lines if line.strip()]\n",
    "        formatted_inputs.append(lines)\n",
    "\n",
    "    return formatted_inputs\n",
    "\n",
    "def generate_input():\n",
    "\n",
    "    with open('sample_data.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    data = json.dumps(data)\n",
    "\n",
    "    parsed_inputs = parse_input(data)\n",
    "    print(parsed_inputs)\n",
    "\n",
    "    output_file = \"translated_sentences_hindi.json\"\n",
    "    translate_sentences_generic(parsed_inputs, output_file, src_lang=\"en_XX\", tgt_lang=\"hi_IN\", model_type=\"mbart\", direction=\"eng_hin\")\n",
    "\n",
    "    print(\"Input generated!!\")\n",
    "    \n",
    "    \n",
    "def generate_input_medium():\n",
    "\n",
    "    with open('sample_medium_25.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    data = json.dumps(data)\n",
    "\n",
    "    parsed_inputs = parse_input(data)\n",
    "    print(parsed_inputs)\n",
    "\n",
    "    output_file = \"translated_sentences_medium_hindi.json\"\n",
    "    translate_sentences_generic(parsed_inputs, output_file, src_lang=\"en_XX\", tgt_lang=\"hi_IN\", model_type=\"mbart\", direction=\"eng_hin\")\n",
    "\n",
    "    print(\"Input generated!!\")\n",
    "    \n",
    "    \n",
    "\n",
    "def get_inputs():\n",
    "    with open('translated_sentences_medium_hindi.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "    print(len(data))\n",
    "    #print(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Function to translate a sample input back to English\n",
    "def translate_sample_to_english(sample_input):\n",
    "    # Wrap the sample input in a list of lists to use with translate_sentences_generic\n",
    "    input_lists = [sample_input]\n",
    "    output_file = \"temp_file.json\"\n",
    "    \n",
    "    # Translate the sample input back to English\n",
    "    translate_sentences_generic(input_lists, output_file, src_lang=\"hi_IN\", tgt_lang=\"en_XX\", model_type=\"mbart\", direction=\"hin_eng\")\n",
    "    \n",
    "    # Load the translated text from the output file\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        translated_data = json.load(f)\n",
    "    \n",
    "    # Extract the translated text\n",
    "    print(\"translated data: \", translated_data)\n",
    "    #translated_text = translated_data\n",
    "    #print(\"Translated text:\", translated_text)\n",
    "    return translated_data\n",
    "\n",
    "\n",
    "def translate_single_sample_to_hindi(diagnosis):\n",
    "    # Wrap the sample input in a list of lists to use with translate_sentences_generic\n",
    "    input_lists = [[diagnosis]]\n",
    "    output_file = \"temp_file_2.json\"\n",
    "    \n",
    "    # Translate the sample input back to English\n",
    "    translate_sentences_generic(input_lists, output_file, src_lang=\"en_XX\", tgt_lang=\"hi_IN\", model_type=\"mbart\", direction=\"eng_hin\")\n",
    "    \n",
    "    # Load the translated text from the output file\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        translated_data = json.load(f)\n",
    "    \n",
    "    # Extract the translated text\n",
    "    print(\"translated data: \", translated_data)\n",
    "    #translated_text = translated_data\n",
    "    #print(\"Translated text:\", translated_text)\n",
    "    return translated_data\n",
    "\n",
    "\n",
    "# Function to translate a sample input back to English\n",
    "def translate_sample_to_hindi(diagnosis,diagnosis_unsloth, diagnosis_mistral):\n",
    "    # Wrap the sample input in a list of lists to use with translate_sentences_generic\n",
    "    input_lists = [[diagnosis],[diagnosis_unsloth], [diagnosis_mistral]]\n",
    "    output_file = \"temp_file_2.json\"\n",
    "    \n",
    "    # Translate the sample input back to English\n",
    "    translate_sentences_generic(input_lists, output_file, src_lang=\"en_XX\", tgt_lang=\"hi_IN\", model_type=\"mbart\", direction=\"eng_hin\")\n",
    "    \n",
    "    # Load the translated text from the output file\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        translated_data = json.load(f)\n",
    "    \n",
    "    # Extract the translated text\n",
    "    print(\"translated data: \", translated_data)\n",
    "    #translated_text = translated_data\n",
    "    #print(\"Translated text:\", translated_text)\n",
    "    return translated_data\n",
    "\n",
    "\n",
    "def process_translated_text_and_get_diagnosis_llama(translated_text):\n",
    "    \"\"\"\n",
    "    Processes the translated text, joins the sentences, and gets the diagnosis from the Medical LLM (Llama).\n",
    "\n",
    "    Args:\n",
    "        translated_text (list): A list of translated sentences.\n",
    "\n",
    "    Returns:\n",
    "        str: The diagnosis from the Medical LLM (Llama).\n",
    "    \"\"\"\n",
    "    # Flatten the list of lists into a single list of strings\n",
    "    flattened_text = [sentence for sublist in translated_text for sentence in sublist]\n",
    "    \n",
    "    # Join the sentences in the translated text\n",
    "    combined_text = ' '.join(flattened_text)\n",
    "    print(f\"Combined text: {combined_text}\")\n",
    "\n",
    "    # Pass the combined text to the Medical LLM and get the diagnosis\n",
    "    diagnosis, diagnosis_json = getDiagnosisLlama(combined_text)\n",
    "    return diagnosis, diagnosis_json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_translated_text_and_get_diagnosis_unsloth(translated_text):\n",
    "    \"\"\"\n",
    "    Processes the translated text, joins the sentences, and gets the diagnosis from the Medical LLM (Unsloth).\n",
    "\n",
    "    Args:\n",
    "        translated_text (list): A list of translated sentences.\n",
    "\n",
    "    Returns:\n",
    "        str: The diagnosis from the Medical LLM (Unsloth).\n",
    "    \"\"\"\n",
    "    # Flatten the list of lists into a single list of strings\n",
    "    flattened_text = [sentence for sublist in translated_text for sentence in sublist]\n",
    "    \n",
    "    # Join the sentences in the translated text\n",
    "    combined_text = ' '.join(flattened_text)\n",
    "    print(f\"Combined text: {combined_text}\")\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    model_path = \"Buddy1421/medical_diagnosis_phi_3-5\"\n",
    "    model, tokenizer = load_unsloth_model_and_tokenizer_phi(model_path, use_safetensors=True)\n",
    "\n",
    "    # Pass the combined text to the Medical LLM and get the diagnosis\n",
    "    diagnosis, diagnosis_json= generate_diagnosis_phi(model, tokenizer, combined_text)\n",
    "    return diagnosis,diagnosis_json\n",
    "\n",
    "\n",
    "def process_translated_text_and_get_diagnosis_mistral(translated_text):\n",
    "    \"\"\"\n",
    "    Processes the translated text, joins the sentences, and gets the diagnosis from the Medical LLM (Mistral).\n",
    "\n",
    "    Args:\n",
    "        translated_text (list): A list of translated sentences.\n",
    "\n",
    "    Returns:\n",
    "        str: The diagnosis from the Medical LLM (Mistral).\n",
    "    \"\"\"\n",
    "    # Flatten the list of lists into a single list of strings\n",
    "    flattened_text = [sentence for sublist in translated_text for sentence in sublist]\n",
    "    \n",
    "    # Join the sentences in the translated text\n",
    "    combined_text = ' '.join(flattened_text)\n",
    "    print(f\"Combined text: {combined_text}\")\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    #model, tokenizer = load_model_mis()\n",
    "\n",
    "    # Pass the combined text to the Medical LLM and get the diagnosis\n",
    "    full_response , diagnosis_json = inference_mis(combined_text, max_length=512)\n",
    "    return full_response, diagnosis_json\n",
    "\n",
    "def format_for_ensemble(mistral_json, llama_json, phi_json):\n",
    "    outputs = {\n",
    "        'mistral' :mistral_json,\n",
    "        'llama': llama_json,\n",
    "        'phi': phi_json,\n",
    "    }\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "621e7431",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"starting demo\")\n",
    "print(\"generating inputs\")\n",
    "#generate_input_medium()\n",
    "print(\"getting inputs\")\n",
    "inputs = get_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e99ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = 8\n",
    "#print(\"Choosing 1 random input : \",example, \" from 25 inputs\")\n",
    "test_input = inputs[example]\n",
    "print(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218e12df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Translating to English\")\n",
    "translated_text = translate_sample_to_english(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa218341",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calling MEDICAL LLM (Llama)\")\n",
    "diagnosis,llama_diagnosis_json = process_translated_text_and_get_diagnosis_llama(translated_text)\n",
    "print(\"Diagnosis:\", diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156e8304",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calling MEDICAL LLM (PHI)\")\n",
    "diagnosis_unsloth, phi_json = process_translated_text_and_get_diagnosis_unsloth(translated_text)\n",
    "print(\"Diagnosis (Unsloth):\", diagnosis_unsloth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aed9626",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calling MEDICAL LLM (Mistral)\")\n",
    "diagnosis_mistral, mistral_diagnosis_json  = process_translated_text_and_get_diagnosis_mistral(translated_text)\n",
    "print(\"Diagnosis (Mistral):\", diagnosis_mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b06826",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs = format_for_ensemble(mistral_diagnosis_json, llama_diagnosis_json, phi_json)\n",
    "print(\"Final English Response: \")\n",
    "ensembled = ensemble_responses(model_outputs)\n",
    "print(ensembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eca285",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_final = translate_single_sample_to_hindi(ensembled)\n",
    "print(f'Final Hindi Response {translated_final}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c679deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4954bcc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef91777315ec425fbffefff91f20ce22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/10.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72c692f007444efaf9490a622c61aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1828178f4434c209c4071dec560ae1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6c0e5fcd274dabb471aacf2ae29664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/992 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f6bf9ef3f14a0bab27f0c8811eb89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/10.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004e2745e01648bfb8db803f57718cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab209fd5c3b94d5ebcc9c7d15baccf72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a25b41aad0472494e3d9f693900470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9ff6972bf5404b99e1fe3100ea5b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/256 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44cf7645c9c4805915a103ea169e8f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/10.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936540a3850b4d86a6c05cee9cad5615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633cabfee55342169b8eb1ba42a527bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f0803cdd044102b9a6fa20a0147ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/992 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f67a03f54de4945be7986264c18ed2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/10.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da159ded2af47a4a7c9ff3f6d2c9486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eacd99219790425e972c0e52bc85d77a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04ca3804b6b45e0856c9c7736543a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"Example 1\")\n",
    "    test_input = inputs[i]\n",
    "    print(test_input)\n",
    "    print(\"Translating to English\")\n",
    "    translated_text = translate_sample_to_english(test_input)\n",
    "    print(\"Calling MEDICAL LLM (Llama)\")\n",
    "    diagnosis,llama_diagnosis_json = process_translated_text_and_get_diagnosis_llama(translated_text)\n",
    "    print(\"Diagnosis:\", diagnosis)\n",
    "    print(\"Calling MEDICAL LLM (PHI)\")\n",
    "    diagnosis_unsloth, phi_json = process_translated_text_and_get_diagnosis_unsloth(translated_text)\n",
    "    print(\"Diagnosis (PHI):\", diagnosis_unsloth)\n",
    "    print(\"Calling MEDICAL LLM (Mistral)\")\n",
    "    diagnosis_mistral, mistral_diagnosis_json  = process_translated_text_and_get_diagnosis_mistral(translated_text)\n",
    "    print(\"Diagnosis (Mistral):\", diagnosis_mistral)\n",
    "    model_outputs = format_for_ensemble(mistral_diagnosis_json, llama_diagnosis_json, phi_json)\n",
    "    print(\"Final English Response: \")\n",
    "    ensembled = ensemble_responses(model_outputs)\n",
    "    print(ensembled)\n",
    "    translated_final = translate_single_sample_to_hindi(ensembled)\n",
    "    print(f'Final Hindi Response {translated_final}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8835f925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddba9ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_TEST_2",
   "language": "python",
   "name": "llm_test_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
