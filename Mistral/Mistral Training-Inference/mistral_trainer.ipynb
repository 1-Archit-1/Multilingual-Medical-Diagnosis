{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145ecda9-4fce-4fb1-a092-ffec5a9fab21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58deba83-a4c7-4208-bad9-e67bc0e76eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files={\"train\": \"train_unsloth2.jsonl\", \"validation\": \"val_unsloth2.jsonl\", \"test\": \"test_unsloth2.jsonl\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc52b904-b66c-4032-9845-504ee26a0260",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"teknium/OpenHermes-2.5-Mistral-7B\",\n",
    "    max_seq_length = 1024,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07d09269-dfc7-4920-8c79-ebfe01ab9c54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2775a2d-ec24-4f10-99bd-010c0cbf2e79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"o_proj\",\"gate_proj\",\"k_proj\", \"v_proj\",\"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\",   \n",
    "    use_gradient_checkpointing = True,\n",
    "    random_state = 3411,\n",
    "    max_seq_length = 1024,\n",
    "    use_rslora = False,  \n",
    "    loftq_config = None, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b1a858-4877-4e19-8d31-fc1569cba76c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "\n",
    "    instruction = 'Perform Diagnosis'\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for inputx, output in zip(inputs, outputs):\n",
    "        text = alpaca_prompt.format(instruction, inputx, output) + tokenizer.eos_token\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9738c1ac-c474-442e-a461-eb2712e97abe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset['train'],\n",
    "    #eval_dataset = dataset['validation'],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 1024,\n",
    "    tokenizer = tokenizer,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 16,\n",
    "        #gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 100,\n",
    "        num_train_epochs = 1,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        output_dir = \"mistral_outputs_v3\",\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        optim = \"adamw_8bit\",\n",
    "        seed = 3407,\n",
    "        learning_rate = 1e-5\n",
    "    ),\n",
    ")\n",
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d4b5609-39a8-47fd-81e6-c5f29c337c69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"mistral_lora_model_v3\")\n",
    "model.save_pretrained_merged(\"mistral_merged_model_v3\", tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbd7cc53-4c7a-4a31-ae94-506f290f50ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model,tokenizer, text, max_length):\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    # Generate predictions\n",
    "    instruction = \"Perform Diagnosis with 1-10 diseases in the format- Differential Diagnosis is:\\s*(.*?),?\\s*and the most likely is **X**\"\n",
    "    instruction2 = \"\"\"Perform diagnosis and return output as a JSON ,following format:\n",
    "        {\n",
    "            'differential_daignosis' // List[str]\n",
    "            'most_likely' //str\n",
    "        }\n",
    "        Don't return any explanation\n",
    "        \"\"\"\n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "        \n",
    "        alpaca_prompt.format(\n",
    "            'Perform Daignosis',\n",
    "            text, # input\n",
    "            \"\", # output - leave this blank for generation!\n",
    "        )\n",
    "    ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens = max_length, use_cache = True)\n",
    "    return tokenizer.batch_decode(outputs)\n",
    "    \n",
    "def load_unsloth_model_and_tokenizer(model_path):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_path,\n",
    "        max_seq_length = 1024,\n",
    "        dtype = None,\n",
    "        load_in_4bit = True,\n",
    "        local_files_only = True,\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "def extract_diagnosis2(input_string):\n",
    "    diff_diag_pattern = r\"'differential_diagnosis': \\[(.*?)\\]\"\n",
    "    most_likely_pattern = r\"'most_likely': '(.*?)'\"\n",
    "    text = input_string\n",
    "    # Extract differential_diagnosis\n",
    "    diff_diag_match = re.search(diff_diag_pattern, text)\n",
    "    if diff_diag_match:\n",
    "        diff_diag_str = diff_diag_match.group(1)\n",
    "        # Split the string into a list of items\n",
    "        differential_diagnosis = [item.strip().strip(\"'\") for item in diff_diag_str.split(\",\")]\n",
    "    else:\n",
    "        differential_diagnosis = []\n",
    "\n",
    "    # Extract most_likely\n",
    "    most_likely_match = re.search(most_likely_pattern, text)\n",
    "    most_likely = most_likely_match.group(1) if most_likely_match else None\n",
    "    print(differential_diagnosis, most_likely)\n",
    "    return most_likely, differential_diagnosis\n",
    "def extract_diagnosis(input_string):\n",
    "    match = re.search(r'\\*\\*(.*?)\\*\\*', input_string)\n",
    "    most_likely = match.group(1) if match else '' # Return the matched disease\n",
    "    \n",
    "    differential_match = re.search(r'Differential Diagnosis is:\\s*(.*?),?\\s*and the most likely is', input_string)\n",
    "    differential_diseases = []\n",
    "    if differential_match:\n",
    "        differential_diseases = [d.strip() for d in differential_match.group(1).split(',')]\n",
    "    \n",
    "    return most_likely, differential_diseases\n",
    "\n",
    "def write_diagnosis_to_file(most_likely, differential, filename):\n",
    "    data = {\n",
    "        \"most_likely_disease\": most_likely,\n",
    "        \"differential_diseases\": differential\n",
    "        }\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(json.dumps(data) + '\\n')\n",
    "        \n",
    "def generate_predictions(test_file, model, tokenizer, output_file, max_length=256):\n",
    "    \"\"\"\n",
    "    Generate predictions for test samples and save to file.\n",
    "    \"\"\"\n",
    "    with open(test_file, 'r') as f:\n",
    "        content = f.read().strip()\n",
    "        json_objects = content.split('}')[:-1]\n",
    "        test_data = [json.loads(f'{obj}}}') for i, obj in enumerate(json_objects)]\n",
    "    n = 0\n",
    "    with open(output_file, 'w') as f:\n",
    "        for item in test_data:\n",
    "            print(f'item {n}')\n",
    "            n+=1\n",
    "            if n<2000 :\n",
    "                continue\n",
    "            human_input = item[\"input\"] \n",
    "            prediction = generate_text(model, tokenizer, human_input, max_length)[0]\n",
    "            print(prediction)\n",
    "            most_likely, differential = extract_diagnosis(prediction)\n",
    "            write_diagnosis_to_file(most_likely, differential, output_file)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a94f5166-5f74-447b-8888-11cbf34816db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.7: Fast Mistral patching. Transformers = 4.46.2.\n",
      "   \\\\   /|    GPU: NVIDIA H100 80GB HBM3. Max memory: 79.097 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1. CUDA = 9.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.11.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "test_file = \"test_unsloth2.jsonl\"  \n",
    "model_output_file = \"model_predictions2.jsonl\"  \n",
    "model_path = 'mistral_lora_model'\n",
    "model, tokenizer = load_unsloth_model_and_tokenizer(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98785b55-de9e-49be-9822-992db40e7aa4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 1654.79 out of 2015.05 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 103.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n",
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits will take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] will take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsloth: The file 'llama.cpp/llama-quantize' or 'llama.cpp/quantize' does not exist.\nBut we expect this file to exist! Maybe the llama.cpp developers changed the name?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained_gguf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1_gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer, quantization_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq4_k_m\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/scratch/.conda/envs/unsloth/lib/python3.11/site-packages/unsloth/save.py:1683\u001b[0m, in \u001b[0;36munsloth_save_pretrained_gguf\u001b[0;34m(self, save_directory, tokenizer, quantization_method, first_conversion, push_to_hub, token, private, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   1680\u001b[0m is_sentencepiece_model \u001b[38;5;241m=\u001b[39m check_if_sentencepiece_model(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;66;03m# Save to GGUF\u001b[39;00m\n\u001b[0;32m-> 1683\u001b[0m all_file_locations, want_full_precision \u001b[38;5;241m=\u001b[39m save_to_gguf(\n\u001b[1;32m   1684\u001b[0m     model_type, model_dtype, is_sentencepiece_model, \n\u001b[1;32m   1685\u001b[0m     new_save_directory, quantization_method, first_conversion, makefile,\n\u001b[1;32m   1686\u001b[0m )\n\u001b[1;32m   1688\u001b[0m \u001b[38;5;66;03m# Save Ollama modelfile\u001b[39;00m\n\u001b[1;32m   1689\u001b[0m modelfile \u001b[38;5;241m=\u001b[39m create_ollama_modelfile(tokenizer, all_file_locations[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/scratch/.conda/envs/unsloth/lib/python3.11/site-packages/unsloth/save.py:996\u001b[0m, in \u001b[0;36msave_to_gguf\u001b[0;34m(model_type, model_dtype, is_sentencepiece, model_directory, quantization_method, first_conversion, _run_installer)\u001b[0m\n\u001b[1;32m    994\u001b[0m     quantize_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama.cpp/llama-quantize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 996\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    997\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: The file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama.cpp/llama-quantize\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama.cpp/quantize\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBut we expect this file to exist! Maybe the llama.cpp developers changed the name?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    999\u001b[0m     )\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;66;03m# See https://github.com/unslothai/unsloth/pull/730\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;66;03m# Filenames changed again!\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unsloth: The file 'llama.cpp/llama-quantize' or 'llama.cpp/quantize' does not exist.\nBut we expect this file to exist! Maybe the llama.cpp developers changed the name?"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_gguf(\"v1_gguf\", tokenizer, quantization_method = \"q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca980f64-55ff-44a8-af3b-f22554808138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate predictions from test file\n",
    "generate_predictions(test_file, model, tokenizer, model_output_file)\n",
    "print(f\"Predictions saved to {model_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35f44fad-198c-467e-b7f9-80bbc3e045f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPerform Daignosis\\n\\n### Input:\\nIs your brain bleeding? Yes\\n\\n### Response:\\nDifferential Diagnosis is: Anemia and the most likely is Anemia<|im_end|>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions through manual prompts\n",
    "prompt = 'Is your brain bleeding? Yes'\n",
    "generate_text(model=model,tokenizer=tokenizer, text=prompt, max_length=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "328fd4c3-9701-4646-b6a9-5e0693d659da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f6be01df-4501-4c92-95a1-c316a8998cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for evaluation\n",
    "\n",
    "def calculate_metrics(pred_set, truth_set):\n",
    "    tp = len(pred_set & truth_set)  # True Positives: intersection of prediction and truth\n",
    "    fp = len(pred_set - truth_set)  # False Positives: in prediction but not in truth\n",
    "    fn = len(truth_set - pred_set)  # False Negatives: in truth but not in prediction\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def load_jsonl(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                # Parse each line as a JSON object and append it to the list\n",
    "                data.append(json.loads(line.strip()))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding line: {line.strip()} - {e}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluation_system(predicted_data, ground_truth_data):\n",
    "    \n",
    "    correct_most_likely = 0\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list =[]\n",
    "    rouge_scores=[]\n",
    "    total_cases = len(ground_truth_data)\n",
    "    for pred, truth in zip(predicted_data, ground_truth_data):\n",
    "        # Evaluate \"Most Likely Disease\"\n",
    "        if pred[\"most_likely_disease\"] == truth[\"most_likely_disease\"]:\n",
    "            correct_most_likely += 1\n",
    "        \n",
    "        # Evaluate \"Differential Diagnosis\"\n",
    "        pred_set = set(pred[\"differential_diseases\"])\n",
    "        truth_set = set(truth[\"differential_diseases\"])\n",
    "        \n",
    "        precision, recall, f1 = calculate_metrics(pred_set, truth_set)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "        # ROUGE score\n",
    "        \n",
    "        predicted =     \" \".join(pred[\"differential_diseases\"]),\n",
    "        ground_truth =     \" \".join(truth[\"differential_diseases\"]),\n",
    "        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "        scores = scorer.score(ground_truth[0], predicted[0])\n",
    "        rouge_scores.append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "    accuracy = correct_most_likely / total_cases\n",
    "    avg_precision = sum(precision_list) / total_cases\n",
    "    avg_recall = sum(recall_list) / total_cases\n",
    "    avg_f1 = sum(f1_list) / total_cases\n",
    "    avg_rouge = sum(rouge_scores) / total_cases\n",
    "    \n",
    "    return {\n",
    "        \"accuracy_for_most_likely\": accuracy,\n",
    "        \"precision_differential\": avg_precision,\n",
    "        \"recall_differential\": avg_recall,\n",
    "        \"f1_differential\": avg_f1,\n",
    "        \"rouge_differential\": avg_rouge,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "03512985-ef24-468d-be33-b25297cc9ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy_most_likely': 0.9958991494532199,\n",
       " 'precision_differential': 0.9725353996770867,\n",
       " 'recall_differential': 0.9751223975634911,\n",
       " 'f1_differential': 0.9702945402271386,\n",
       " 'rouge_differential': 0.9448142731543822}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_data = load_jsonl('model_predictions.jsonl')\n",
    "ground_truth_data = load_jsonl('test_evaluation.jsonl')\n",
    "evaluation_system(predicted_data,ground_truth_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fea1e7-7e86-4443-929c-ba604029438e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-unsloth]",
   "language": "python",
   "name": "conda-env-.conda-unsloth-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
