{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39a98807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "~/scratch/hf-cache\n"
     ]
    }
   ],
   "source": [
    "SHOULD_TRAIN = False\n",
    "#SHOULD_TRAIN = True\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "\n",
    "# disable Weights and Biases\n",
    "os.environ['WANDB_DISABLED']=\"true\"\n",
    "os.environ[\"HF_HOME\"] = \"~/scratch/hf-cache\"\n",
    "token=\"\"\n",
    "print(os.environ['WANDB_DISABLED'])  # Should output \"true\"\n",
    "print(os.environ['HF_HOME'])  # Should output \"~/scratch/hf-cache\"\n",
    "\n",
    "#output_file = open('logger_tests.log', 'w')\n",
    "#sys.stdout = output_file\n",
    "#sys.stderr = output_file\n",
    "\n",
    "LANG_TOKEN_MAPPING = {\n",
    "    'hi': '',\n",
    "    'en': ''\n",
    "}\n",
    "max_seq_len = 25\n",
    "last_print_time = time.time()\n",
    "model_path = \"./models/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "944c0409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "~/scratch/hf-cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to ./nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to ./nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to ./nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to ./nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to ./nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to ./nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to ./nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to ./nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to ./nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to ./nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to ./nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to ./nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to ./nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to ./nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to ./nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to ./nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to ./nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to ./nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to ./nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to ./nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to ./nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to ./nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to ./nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to ./nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to ./nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to ./nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to ./nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to ./nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to ./nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to ./nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to ./nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to ./nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to ./nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to ./nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to ./nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to ./nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to ./nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to ./nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to ./nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to ./nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to ./nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to ./nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to ./nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to ./nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to ./nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to ./nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to ./nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to ./nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to ./nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to ./nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to ./nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to ./nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to ./nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to ./nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to ./nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to ./nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to ./nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to ./nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to ./nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to ./nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to ./nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to ./nltk_data...\n",
      "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to ./nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to ./nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to ./nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to ./nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to ./nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to ./nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to ./nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to ./nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to ./nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to ./nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to ./nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to ./nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to ./nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to ./nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to ./nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to ./nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to ./nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to ./nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets_json to ./nltk_data...\n",
      "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to ./nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to ./nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to ./nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to ./nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to ./nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to ./nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to ./nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     ./nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to ./nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to ./nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to ./nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to ./nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to ./nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to ./nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to ./nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to ./nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to ./nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to ./nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to ./nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to ./nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to ./nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.11: Fast Mistral patching. Transformers:4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe. Max memory: 79.256 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.11.11 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules and functions from translate_codebase.py\n",
    "\n",
    "from translate_codebase import (\n",
    "    # Environment variables and settings\n",
    "    # Generic Helper functions\n",
    "    clock_begin,\n",
    "    clock_end,\n",
    "    read_token_and_login,\n",
    "    print_gpu_utilization,\n",
    "    clear_cuda_cache,\n",
    "    # Metrics Functions\n",
    "    evaluate_translations_bertscore,\n",
    "    evaluate_translations_rouge,\n",
    "    evaluate_translations_bleu,\n",
    "    evaluate_translations_meteor,\n",
    "    # Common Data Preprocessing Functions\n",
    "    filter_sentences,\n",
    "    get_reduced_dataset,\n",
    "    prepare_test_data,\n",
    "    perform_translation_testing,\n",
    "    # MBART related functions\n",
    "    get_pretrained_mbart_large_50_many_to_many_mmt,\n",
    "    preprocess_function_mbart,\n",
    "    prepare_model_for_training_mbart,\n",
    "    fine_tune_and_save_mbart,\n",
    "    load_fine_tuned_model_mbart,\n",
    "    translate_text_mbart,\n",
    "    # MT5 related functions\n",
    "    get_pretrained_mt5_small,\n",
    "    config_mt5_small,\n",
    "    encode_input_str_mt5_small,\n",
    "    encode_target_str_mt5_small,\n",
    "    process_translation_list_mt5_small,\n",
    "    format_translation_data_mt5_small,\n",
    "    transform_batch_mt5_small,\n",
    "    fine_tune_and_save_model_mt5_small,\n",
    "    eval_model_mt5_small,\n",
    "    save_fine_tuned_model_mt5_small,\n",
    "    load_fine_tuned_model_mt5_small,\n",
    "    translate_text_mt5_small\n",
    ")\n",
    "\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "from team_files.akshay import getDiagnosisLlama\n",
    "from team_files.aditya import load_unsloth_model_and_tokenizer_phi, generate_diagnosis_phi\n",
    "from team_files.archit import load_model_mis, generate_text_mis, inference_mis\n",
    "from team_files.ensemble import ensemble_responses\n",
    "\n",
    "#from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f2c6a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mbart_eng_hin_huggingface(force_download=True):\n",
    "\n",
    "    model_path = \"sathvikaithalkp456/mbart_fine_tuned_eng_hin\"\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(model_path, force_download = force_download)\n",
    "    model = MBartForConditionalGeneration.from_pretrained(model_path, force_download = force_download)\n",
    "    tokenizer.src_lang = \"en_XX\"\n",
    "    tokenizer.tgt_lang = \"hi_IN\"\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_mbart_hin_eng_huggingface(force_download=True):\n",
    "    revision = \"master\"\n",
    "    model_path = \"sathvikaithalkp456/mbart_fine_tuned_hin_eng\"\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(model_path, force_download = force_download, revision=revision)\n",
    "    model = MBartForConditionalGeneration.from_pretrained(model_path, force_download = force_download, revision=revision)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_mt5_small_eng_hin_huggingface(force_download=True):\n",
    "\n",
    "    revision = \"master\"\n",
    "    model_path = 'sathvikaithalkp456/mbart_fine_tuned_eng_hin'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, force_download = force_download, revision=revision)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path, force_download = force_download, revision=revision)\n",
    "    model = model.cuda()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_mt5_small_hin_eng_huggingface(force_download=True):\n",
    "\n",
    "    revision = \"master\"\n",
    "    model_path = 'sathvikaithalkp456/mbart_fine_tuned_hin_eng'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, force_download = force_download, revision=revision)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path, force_download = force_download, revision=revision)\n",
    "    model = model.cuda()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def translate_text_generic(model, tokenizer, input_text, src_lang, tgt_lang, model_type=\"mbart\"):\n",
    "    \"\"\"\n",
    "    Translates a given text from the source language to the target language.\n",
    "\n",
    "    Args:\n",
    "        model: The translation model.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "        input_text: The input text to be translated.\n",
    "        src_lang: The source language code.\n",
    "        tgt_lang: The target language code.\n",
    "        model_type: The type of model (\"mbart\" or \"mt5\").\n",
    "\n",
    "    Returns:\n",
    "        str: The translated text.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    if model_type == \"mbart\":\n",
    "        # Tokenize the input text with padding and truncation\n",
    "        tokenizer.src_lang = src_lang\n",
    "        encoded_input = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "        # Generate translation\n",
    "        generated_tokens = model.generate(\n",
    "            **encoded_input,\n",
    "            forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang]\n",
    "        )\n",
    "\n",
    "        # Decode the generated tokens\n",
    "        translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    else:  # mt5\n",
    "        # Encode the input text with padding and truncation\n",
    "        input_ids = encode_input_str_mt5_small(\n",
    "            text=input_text,\n",
    "            target_lang=tgt_lang,\n",
    "            tokenizer=tokenizer,\n",
    "            seq_len=model.config.max_length,\n",
    "            lang_token_map=LANG_TOKEN_MAPPING\n",
    "        )\n",
    "        input_ids = input_ids.unsqueeze(0).to(device)\n",
    "\n",
    "        # Generate the translation\n",
    "        output_ids = model.generate(input_ids, num_beams=5, max_length=model.config.max_length, early_stopping=True)\n",
    "\n",
    "        # Decode the generated tokens\n",
    "        translated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    #print(\"Translated text:\", translated_text)\n",
    "    return translated_text\n",
    "\n",
    "\n",
    "def translate_sentences_generic(input_lists, output_file, src_lang, tgt_lang, model_type=\"mbart\", direction=\"eng_hin\"):\n",
    "    \"\"\"\n",
    "    Translates each sentence in the input lists to the target language and saves the translated sentences to a file.\n",
    "\n",
    "    Args:\n",
    "        input_lists (list): A list of lists, where each inner list contains sentences to be translated.\n",
    "        output_file (str): The path to the output file where translated sentences will be saved.\n",
    "        src_lang (str): The source language code.\n",
    "        tgt_lang (str): The target language code.\n",
    "        model_type (str): The type of model to use for translation (\"mbart\" or \"mt5\").\n",
    "        direction (str): The direction of translation (\"eng_hin\" or \"hin_eng\").\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if model_type == \"mbart\":\n",
    "        if direction == \"eng_hin\":\n",
    "            model, tokenizer = get_mbart_eng_hin_huggingface()\n",
    "        else:  # hin_eng\n",
    "            model, tokenizer = get_mbart_hin_eng_huggingface()\n",
    "    else:  # mt5\n",
    "        if direction == \"eng_hin\":\n",
    "            model, tokenizer = get_mt5_small_eng_hin_huggingface()\n",
    "        else:  # hin_eng\n",
    "            model, tokenizer = get_mt5_small_hin_eng_huggingface()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    translated_data = []\n",
    "\n",
    "    for input_list in input_lists:\n",
    "        translated_list = []\n",
    "        for sentence in input_list:\n",
    "            translated_sentence = translate_text_generic(model, tokenizer, sentence, src_lang, tgt_lang, model_type=model_type)\n",
    "            translated_list.append(translated_sentence)\n",
    "        translated_data.append(translated_list)\n",
    "\n",
    "    # Save the translated sentences to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(translated_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def parse_input(data):\n",
    "    \"\"\"\n",
    "    Parses the input data, replaces 'Y' with 'Yes' and 'N' with 'No', and returns it as a list of formatted inputs.\n",
    "\n",
    "    Args:\n",
    "        data (str): The input data in JSON format.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of formatted inputs.\n",
    "    \"\"\"\n",
    "    parsed_data = json.loads(data)\n",
    "    formatted_inputs = []\n",
    "\n",
    "    for item in parsed_data:\n",
    "        input_text = item['input']\n",
    "        # Replace ' Y ' with ' Yes ' and ' N ' with ' No '\n",
    "        input_text = input_text.replace(' Y ', ' Yes ').replace(' N ', ' No ')\n",
    "        # Split the input text into individual lines based on delimiters\n",
    "        lines = re.split(r'[;.]', input_text)\n",
    "        # Strip leading and trailing whitespace from each line and filter out empty lines\n",
    "        lines = [line.strip() for line in lines if line.strip()]\n",
    "        formatted_inputs.append(lines)\n",
    "\n",
    "    return formatted_inputs\n",
    "\n",
    "def generate_input():\n",
    "\n",
    "    with open('sample_data.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    data = json.dumps(data)\n",
    "\n",
    "    parsed_inputs = parse_input(data)\n",
    "    print(parsed_inputs)\n",
    "\n",
    "    output_file = \"translated_sentences_hindi.json\"\n",
    "    translate_sentences_generic(parsed_inputs, output_file, src_lang=\"en_XX\", tgt_lang=\"hi_IN\", model_type=\"mbart\", direction=\"eng_hin\")\n",
    "\n",
    "    print(\"Input generated!!\")\n",
    "\n",
    "def get_inputs():\n",
    "    with open('translated_sentences_hindi.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "    print(len(data))\n",
    "    #print(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Function to translate a sample input back to English\n",
    "def translate_sample_to_english(sample_input):\n",
    "    # Wrap the sample input in a list of lists to use with translate_sentences_generic\n",
    "    input_lists = [sample_input]\n",
    "    output_file = \"temp_file.json\"\n",
    "    \n",
    "    # Translate the sample input back to English\n",
    "    translate_sentences_generic(input_lists, output_file, src_lang=\"hi_IN\", tgt_lang=\"en_XX\", model_type=\"mbart\", direction=\"hin_eng\")\n",
    "    \n",
    "    # Load the translated text from the output file\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        translated_data = json.load(f)\n",
    "    \n",
    "    # Extract the translated text\n",
    "    print(\"translated data: \", translated_data)\n",
    "    #translated_text = translated_data\n",
    "    #print(\"Translated text:\", translated_text)\n",
    "    return translated_data\n",
    "\n",
    "\n",
    "def translate_single_sample_to_hindi(diagnosis):\n",
    "    # Wrap the sample input in a list of lists to use with translate_sentences_generic\n",
    "    input_lists = [[diagnosis]]\n",
    "    output_file = \"temp_file_2.json\"\n",
    "    \n",
    "    # Translate the sample input back to English\n",
    "    translate_sentences_generic(input_lists, output_file, src_lang=\"en_XX\", tgt_lang=\"hi_IN\", model_type=\"mbart\", direction=\"eng_hin\")\n",
    "    \n",
    "    # Load the translated text from the output file\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        translated_data = json.load(f)\n",
    "    \n",
    "    # Extract the translated text\n",
    "    print(\"translated data: \", translated_data)\n",
    "    #translated_text = translated_data\n",
    "    #print(\"Translated text:\", translated_text)\n",
    "    return translated_data\n",
    "\n",
    "\n",
    "# Function to translate a sample input back to English\n",
    "def translate_sample_to_hindi(diagnosis,diagnosis_unsloth, diagnosis_mistral):\n",
    "    # Wrap the sample input in a list of lists to use with translate_sentences_generic\n",
    "    input_lists = [[diagnosis],[diagnosis_unsloth], [diagnosis_mistral]]\n",
    "    output_file = \"temp_file_2.json\"\n",
    "    \n",
    "    # Translate the sample input back to English\n",
    "    translate_sentences_generic(input_lists, output_file, src_lang=\"en_XX\", tgt_lang=\"hi_IN\", model_type=\"mbart\", direction=\"eng_hin\")\n",
    "    \n",
    "    # Load the translated text from the output file\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        translated_data = json.load(f)\n",
    "    \n",
    "    # Extract the translated text\n",
    "    print(\"translated data: \", translated_data)\n",
    "    #translated_text = translated_data\n",
    "    #print(\"Translated text:\", translated_text)\n",
    "    return translated_data\n",
    "\n",
    "\n",
    "def process_translated_text_and_get_diagnosis_llama(translated_text):\n",
    "    \"\"\"\n",
    "    Processes the translated text, joins the sentences, and gets the diagnosis from the Medical LLM (Llama).\n",
    "\n",
    "    Args:\n",
    "        translated_text (list): A list of translated sentences.\n",
    "\n",
    "    Returns:\n",
    "        str: The diagnosis from the Medical LLM (Llama).\n",
    "    \"\"\"\n",
    "    # Flatten the list of lists into a single list of strings\n",
    "    flattened_text = [sentence for sublist in translated_text for sentence in sublist]\n",
    "    \n",
    "    # Join the sentences in the translated text\n",
    "    combined_text = ' '.join(flattened_text)\n",
    "    print(f\"Combined text: {combined_text}\")\n",
    "\n",
    "    # Pass the combined text to the Medical LLM and get the diagnosis\n",
    "    diagnosis, diagnosis_json = getDiagnosisLlama(combined_text)\n",
    "    return diagnosis, diagnosis_json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_translated_text_and_get_diagnosis_unsloth(translated_text):\n",
    "    \"\"\"\n",
    "    Processes the translated text, joins the sentences, and gets the diagnosis from the Medical LLM (Unsloth).\n",
    "\n",
    "    Args:\n",
    "        translated_text (list): A list of translated sentences.\n",
    "\n",
    "    Returns:\n",
    "        str: The diagnosis from the Medical LLM (Unsloth).\n",
    "    \"\"\"\n",
    "    # Flatten the list of lists into a single list of strings\n",
    "    flattened_text = [sentence for sublist in translated_text for sentence in sublist]\n",
    "    \n",
    "    # Join the sentences in the translated text\n",
    "    combined_text = ' '.join(flattened_text)\n",
    "    print(f\"Combined text: {combined_text}\")\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    model_path = \"Buddy1421/medical_diagnosis_phi_3-5\"\n",
    "    model, tokenizer = load_unsloth_model_and_tokenizer_phi(model_path, use_safetensors=True)\n",
    "\n",
    "    # Pass the combined text to the Medical LLM and get the diagnosis\n",
    "    diagnosis, diagnosis_json= generate_diagnosis_phi(model, tokenizer, combined_text)\n",
    "    return diagnosis,diagnosis_json\n",
    "\n",
    "\n",
    "def process_translated_text_and_get_diagnosis_mistral(translated_text):\n",
    "    \"\"\"\n",
    "    Processes the translated text, joins the sentences, and gets the diagnosis from the Medical LLM (Mistral).\n",
    "\n",
    "    Args:\n",
    "        translated_text (list): A list of translated sentences.\n",
    "\n",
    "    Returns:\n",
    "        str: The diagnosis from the Medical LLM (Mistral).\n",
    "    \"\"\"\n",
    "    # Flatten the list of lists into a single list of strings\n",
    "    flattened_text = [sentence for sublist in translated_text for sentence in sublist]\n",
    "    \n",
    "    # Join the sentences in the translated text\n",
    "    combined_text = ' '.join(flattened_text)\n",
    "    print(f\"Combined text: {combined_text}\")\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    #model, tokenizer = load_model_mis()\n",
    "\n",
    "    # Pass the combined text to the Medical LLM and get the diagnosis\n",
    "    full_response , diagnosis_json = inference_mis(combined_text, max_length=512)\n",
    "    return full_response, diagnosis_json\n",
    "\n",
    "def format_for_ensemble(mistral_json, llama_json, phi_json):\n",
    "    outputs = {\n",
    "        'mistral' :mistral_json,\n",
    "        'llama': llama_json,\n",
    "        'phi': phi_json,\n",
    "    }\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "621e7431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting demo\n",
      "getting inputs\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "print(\"starting demo\")\n",
    "#print(\"generating inputs\")\n",
    "#generate_input()\n",
    "print(\"getting inputs\")\n",
    "inputs = get_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "573e99ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‡§∞‡•ã‡§ó‡•Ä ‡§ï‡§æ ‡§Ü‡§Ø‡•Å 41 ‡§π‡•à, ‡§≤‡§ø‡§Ç‡§ó F ‡§π‡•à', '‡§™‡•Ç‡§∞‡•ç‡§µ‡§ú‡§É ‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™ ‡§ö‡§æ‡§∞ ‡§Ø‡§æ ‡§Ö‡§ß‡§ø‡§ï ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•á ‡§∏‡§æ‡§• ‡§∞‡§π‡§§‡•á ‡§π‡•à‡§Ç? ‡§π‡§æ‡§Å', '‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™ ‡§è‡§ï ‡§¶‡§ø‡§µ‡§∏‡§æ‡§≤‡§Ø ‡§Æ‡•á‡§Ç ‡§Ü‡§§‡•á ‡§π‡•à‡§Ç ‡§Ø‡§æ ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç? ‡§π‡§æ‡§Å', '‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™‡§®‡•á ‡§™‡§ø‡§õ‡§≤‡•á 2 ‡§π‡§´‡•ç‡§§‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§†‡§Ç‡§° ‡§Æ‡§π‡§∏‡•Ç‡§∏ ‡§ï‡•Ä ‡§π‡•à? ‡§π‡§æ‡§Å', '‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™‡§®‡•á ‡§™‡§ø‡§õ‡§≤‡•á 4 ‡§π‡§´‡•ç‡§§‡•á ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§∂ ‡§∏‡•á ‡§¨‡§æ‡§π‡§∞ ‡§Ø‡§æ‡§§‡•ç‡§∞‡§æ ‡§ï‡•Ä ‡§π‡•à? ‡§®‡§π‡•Ä‡§Ç', '‡§≤‡§ï‡•ç‡§∑‡§£‡§É ‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™‡§®‡•á ‡§¶‡•á‡§ñ‡§æ ‡§π‡•à ‡§ï‡§ø ‡§Ü‡§™‡§ï‡•Ä ‡§Ü‡§µ‡§æ‡§ú ‡§ó‡§π‡§∞‡•Ä, ‡§π‡§≤‡•ç‡§ï‡•Ä ‡§Ø‡§æ ‡§ó‡§π‡§∞‡•Ä ‡§π‡•ã ‡§ó‡§à ‡§π‡•à? ‡§π‡§æ‡§Å', '‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™‡§ï‡•á ‡§™‡§∞‡§æ‡§Æ‡§∞‡•ç‡§∂ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§™‡§ï‡•á ‡§ï‡§æ‡§∞‡§£ ‡§∏‡•á ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§ï‡§π‡•Ä‡§Ç ‡§¶‡§∞‡•ç‡§¶ ‡§π‡•à? ‡§π‡§æ‡§Å', '‡§Ö‡§™‡§®‡•á ‡§¶‡§∞‡•ç‡§¶ ‡§ï‡•ã ‡§ö‡§ø‡§π‡•ç‡§®‡§ø‡§§ ‡§ï‡§∞‡•á‡§Ç‡§É ‡§∏‡§Ç‡§µ‡•á‡§¶‡§®‡§∂‡•Ä‡§≤', '‡§Ö‡§™‡§®‡•á ‡§¶‡§∞‡•ç‡§¶ ‡§ï‡•ã ‡§ö‡§ø‡§π‡•ç‡§®‡§ø‡§§ ‡§ï‡§∞‡•á‡§Ç‡§É ‡§ú‡§≤‡§®‡§æ', '‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™ ‡§ï‡§π‡•Ä‡§Ç ‡§¶‡§∞‡•ç‡§¶ ‡§Æ‡§π‡§∏‡•Ç‡§∏ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç?', '‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™ ‡§ï‡§π‡•Ä‡§Ç ‡§¶‡§∞‡•ç‡§¶ ‡§Æ‡§π‡§∏‡•Ç‡§∏ ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç? ‡§∂‡•ç‡§µ‡§æ‡§∏‡§™‡•ç‡§∞‡§£‡§æ‡§≤‡•Ä', '‡§¶‡§∞‡•ç‡§¶ ‡§ï‡§ø‡§§‡§®‡•Ä ‡§§‡•Ä‡§µ‡•ç‡§∞ ‡§π‡•à?', '‡§ï‡•ç‡§Ø‡§æ ‡§¶‡§∞‡•ç‡§¶ ‡§¶‡•Ç‡§∏‡§∞‡•á ‡§∏‡•ç‡§•‡§æ‡§® ‡§™‡§∞ ‡§µ‡§ø‡§ï‡§ø‡§∞‡§£ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à? ‡§ï‡§π‡•Ä‡§Ç ‡§®‡§π‡•Ä‡§Ç', '‡§¶‡§∞‡•ç‡§¶ ‡§ï‡•Ä ‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§ï‡§ø‡§§‡§®‡•Ä ‡§∏‡§π‡•Ä ‡§π‡•à? 5', '‡§¶‡§∞‡•ç‡§¶ ‡§ï‡§ø‡§§‡§®‡•Ä ‡§ú‡§≤‡•ç‡§¶‡•Ä ‡§™‡•ç‡§∞‡§ï‡§ü ‡§π‡•Å‡§Ü?', '‡§Ü‡§™‡§ï‡•ã ‡§¨‡•Å‡§ñ‡§æ‡§∞ ‡§π‡•à (‡§Ø‡§æ ‡§§‡•ã ‡§Æ‡§π‡§∏‡•Ç‡§∏ ‡§ï‡§ø‡§Ø‡§æ ‡§Ø‡§æ ‡§è‡§ï ‡§•‡§∞‡•ç‡§Æ‡•ã‡§Æ‡•Ä‡§ü‡§∞ ‡§∏‡•á ‡§Æ‡§æ‡§™‡§æ)? ‡§π‡§æ‡§Å', '‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™‡§®‡•á ‡§¶‡•á‡§ñ‡§æ ‡§π‡•à ‡§ï‡§ø ‡§Ü‡§™‡§ï‡•Ä ‡§Ü‡§µ‡§æ‡§ú ‡§ï‡•Ä ‡§ß‡•ç‡§µ‡§®‡§ø ‡§ó‡§π‡§∞‡•Ä, ‡§π‡§≤‡•ç‡§ï‡•Ä ‡§Ø‡§æ ‡§ß‡•Å‡§Ç‡§ß‡§≤‡•Ä ‡§π‡•ã ‡§ó‡§à ‡§π‡•à? ‡§π‡§æ‡§Å']\n"
     ]
    }
   ],
   "source": [
    "example = 8\n",
    "#print(\"Choosing 1 random input : \",example, \" from 25 inputs\")\n",
    "test_input = inputs[example]\n",
    "print(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "218e12df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating to English\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88e278f87d54a69a489d1ad2f64f611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/10.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b8f619a1214bb1bc69cb77001f7e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679eb7ca6bb546338eb9aa87e73fe698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8e1080493a4fa4baad19f9e8ff0b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/992 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e682b82714d845bca4b2a785eef6b744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/10.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "250d68e224904375a4967eb9d3ee6d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8c3fccd72849dcbbb1acfd42be358b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3c1d90e0744470a3a6a31fc293a44d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1214b19456ef435abe49c5cce3f41acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/256 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translated data:  [['Age of the patient: 41, gender: F', 'Earlier: Do you live with four or more people? Yes', 'Do you come to or work in a day shop? Yes.', 'Did you feel cold in the last two weeks? Yes.', 'Have you travelled outside the country in the last 4 weeks? No', 'Symptoms: Have you noticed that your voice is getting louder, softer or louder? Yes', 'Is there any pain related to your cause for advice? Yes', 'Mark your pain: Sensitive', 'Mark your pain: burning', 'Do you feel any pain anywhere?', 'Do you feel any pain anywhere?', 'How severe is the pain?', 'Does the pain radiate elsewhere? Nowhere', 'How accurate is the pain condition? 5', 'How early did the pain appear?', 'Do you have fever (either felt or measured by a thermometer)? Yes', 'Have you noticed that the sound of your voice is getting louder or fainter? Yes.']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Translating to English\")\n",
    "translated_text = translate_sample_to_english(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa218341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling MEDICAL LLM (Llama)\n",
      "Combined text: Age of the patient: 41, gender: F Earlier: Do you live with four or more people? Yes Do you come to or work in a day shop? Yes. Did you feel cold in the last two weeks? Yes. Have you travelled outside the country in the last 4 weeks? No Symptoms: Have you noticed that your voice is getting louder, softer or louder? Yes Is there any pain related to your cause for advice? Yes Mark your pain: Sensitive Mark your pain: burning Do you feel any pain anywhere? Do you feel any pain anywhere? How severe is the pain? Does the pain radiate elsewhere? Nowhere How accurate is the pain condition? 5 How early did the pain appear? Do you have fever (either felt or measured by a thermometer)? Yes Have you noticed that the sound of your voice is getting louder or fainter? Yes.\n",
      "Loading the model...\n",
      "Model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnosis: Differential diagnosis is: Viral pharyngitis, URTI, Bronchitis, Acute laryngitis, Possible NSTEMI / STEMI, Influenza, Unstable angina, Tuberculosis, Stable angina and the Disease can be Viral pharyngitis \n"
     ]
    }
   ],
   "source": [
    "print(\"Calling MEDICAL LLM (Llama)\")\n",
    "diagnosis,llama_diagnosis_json = process_translated_text_and_get_diagnosis_llama(translated_text)\n",
    "print(\"Diagnosis:\", diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "156e8304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling MEDICAL LLM (PHI)\n",
      "Combined text: Age of the patient: 41, gender: F Earlier: Do you live with four or more people? Yes Do you come to or work in a day shop? Yes. Did you feel cold in the last two weeks? Yes. Have you travelled outside the country in the last 4 weeks? No Symptoms: Have you noticed that your voice is getting louder, softer or louder? Yes Is there any pain related to your cause for advice? Yes Mark your pain: Sensitive Mark your pain: burning Do you feel any pain anywhere? Do you feel any pain anywhere? How severe is the pain? Does the pain radiate elsewhere? Nowhere How accurate is the pain condition? 5 How early did the pain appear? Do you have fever (either felt or measured by a thermometer)? Yes Have you noticed that the sound of your voice is getting louder or fainter? Yes.\n",
      "==((====))==  Unsloth 2024.11.11: Fast Llama patching. Transformers:4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe. Max memory: 79.256 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Diagnosis (Unsloth): Differential Diagnosis is: Acute laryngitis, Bronchitis, Chagas, Viral pharyngitis and the Disease can be Acute laryngitis\n"
     ]
    }
   ],
   "source": [
    "print(\"Calling MEDICAL LLM (PHI)\")\n",
    "diagnosis_unsloth, phi_json = process_translated_text_and_get_diagnosis_unsloth(translated_text)\n",
    "print(\"Diagnosis (Unsloth):\", diagnosis_unsloth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aed9626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling MEDICAL LLM (Mistral)\n",
      "Combined text: Age of the patient: 41, gender: F Earlier: Do you live with four or more people? Yes Do you come to or work in a day shop? Yes. Did you feel cold in the last two weeks? Yes. Have you travelled outside the country in the last 4 weeks? No Symptoms: Have you noticed that your voice is getting louder, softer or louder? Yes Is there any pain related to your cause for advice? Yes Mark your pain: Sensitive Mark your pain: burning Do you feel any pain anywhere? Do you feel any pain anywhere? How severe is the pain? Does the pain radiate elsewhere? Nowhere How accurate is the pain condition? 5 How early did the pain appear? Do you have fever (either felt or measured by a thermometer)? Yes Have you noticed that the sound of your voice is getting louder or fainter? Yes.\n",
      "==((====))==  Unsloth 2024.11.11: Fast Mistral patching. Transformers:4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe. Max memory: 79.256 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Diagnosis (Mistral): Differential Diagnosis is: Acute laryngitis, Chagas and the most likely is **Acute laryngitis**\n"
     ]
    }
   ],
   "source": [
    "print(\"Calling MEDICAL LLM (Mistral)\")\n",
    "diagnosis_mistral, mistral_diagnosis_json  = process_translated_text_and_get_diagnosis_mistral(translated_text)\n",
    "print(\"Diagnosis (Mistral):\", diagnosis_mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43b06826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final English Response: \n",
      "Differential diagnosis is: Acute laryngitis, Chagas, Viral pharyngitis, URTI, Bronchitis and the most likely diagnosis is Acute laryngitis\n"
     ]
    }
   ],
   "source": [
    "model_outputs = format_for_ensemble(mistral_diagnosis_json, llama_diagnosis_json, phi_json)\n",
    "print(\"Final English Response: \")\n",
    "ensembled = ensemble_responses(model_outputs)\n",
    "print(ensembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73eca285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e3d0e2e48094948ad1667b847214cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/10.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80dc4c46297c4f2b9e12b90e02a32437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03bea41465d74eae8e7f27e2761f9bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b290adb24c0499b87d3a35c042b42d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/992 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b9c32388fe4effa76bbad0b74d725c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/10.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165906bb294a4bdda29f2d0308d9c66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180961d7bf2e49ff8149d092a223a88f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c42af9048d514f46afbc67a18778f0b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6eda58f653e430bb3dd9bca25316850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/256 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translated data:  [['‡§µ‡§ø‡§≠‡•á‡§¶‡•Ä ‡§®‡§ø‡§¶‡§æ‡§® ‡§π‡•à‡§É ‡§§‡•Ä‡§µ‡•ç‡§∞ ‡§≤‡•à‡§∞‡§ø‡§Ç‡§ó‡•Ä‡§ü‡§ø‡§∏, ‡§ö‡§æ‡§ó‡§æ‡§∏, ‡§µ‡§ø‡§∑‡§æ‡§£‡•Å ‡§´‡•ç‡§∞‡•á‡§∞‡§ø‡§Ç‡§ó‡•Ä‡§ü‡§ø‡§∏, ‡§Ø‡•Ç‡§Ü‡§∞‡§ü‡•Ä‡§Ü‡§à, ‡§¨‡•ç‡§∞‡•ã‡§®‡•ç‡§ö‡§ø‡§ü‡§ø‡§∏ ‡§î‡§∞ ‡§∏‡§∞‡•ç‡§µ‡§æ‡§ß‡§ø‡§ï ‡§∏‡§Ç‡§≠‡§æ‡§µ‡§ø‡§§ ‡§®‡§ø‡§¶‡§æ‡§® ‡§π‡•à ‡§§‡•Ä‡§µ‡•ç‡§∞ ‡§≤‡•à‡§∞‡§ø‡§Ç‡§ó‡•Ä‡§ü‡§ø‡§∏']]\n",
      "Final Hindi Response [['‡§µ‡§ø‡§≠‡•á‡§¶‡•Ä ‡§®‡§ø‡§¶‡§æ‡§® ‡§π‡•à‡§É ‡§§‡•Ä‡§µ‡•ç‡§∞ ‡§≤‡•à‡§∞‡§ø‡§Ç‡§ó‡•Ä‡§ü‡§ø‡§∏, ‡§ö‡§æ‡§ó‡§æ‡§∏, ‡§µ‡§ø‡§∑‡§æ‡§£‡•Å ‡§´‡•ç‡§∞‡•á‡§∞‡§ø‡§Ç‡§ó‡•Ä‡§ü‡§ø‡§∏, ‡§Ø‡•Ç‡§Ü‡§∞‡§ü‡•Ä‡§Ü‡§à, ‡§¨‡•ç‡§∞‡•ã‡§®‡•ç‡§ö‡§ø‡§ü‡§ø‡§∏ ‡§î‡§∞ ‡§∏‡§∞‡•ç‡§µ‡§æ‡§ß‡§ø‡§ï ‡§∏‡§Ç‡§≠‡§æ‡§µ‡§ø‡§§ ‡§®‡§ø‡§¶‡§æ‡§® ‡§π‡•à ‡§§‡•Ä‡§µ‡•ç‡§∞ ‡§≤‡•à‡§∞‡§ø‡§Ç‡§ó‡•Ä‡§ü‡§ø‡§∏']]\n"
     ]
    }
   ],
   "source": [
    "translated_final = translate_single_sample_to_hindi(ensembled)\n",
    "print(f'Final Hindi Response {translated_final}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c679deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4954bcc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8835f925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddba9ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_TEST_2",
   "language": "python",
   "name": "llm_test_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
